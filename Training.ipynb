{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e69e5c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:18:32.331732Z",
     "iopub.status.busy": "2025-10-13T14:18:32.331454Z",
     "iopub.status.idle": "2025-10-13T14:18:32.452441Z",
     "shell.execute_reply": "2025-10-13T14:18:32.451634Z"
    },
    "papermill": {
     "duration": 0.128899,
     "end_time": "2025-10-13T14:18:32.453826",
     "exception": false,
     "start_time": "2025-10-13T14:18:32.324927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.12\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "152e8fd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:18:32.464880Z",
     "iopub.status.busy": "2025-10-13T14:18:32.464657Z",
     "iopub.status.idle": "2025-10-13T14:18:32.467793Z",
     "shell.execute_reply": "2025-10-13T14:18:32.467174Z"
    },
    "papermill": {
     "duration": 0.009924,
     "end_time": "2025-10-13T14:18:32.469122",
     "exception": false,
     "start_time": "2025-10-13T14:18:32.459198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/kaggle/input/monodtr-library-ver2/MonoDTR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "367e9d82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:18:32.479701Z",
     "iopub.status.busy": "2025-10-13T14:18:32.479502Z",
     "iopub.status.idle": "2025-10-13T14:18:44.250363Z",
     "shell.execute_reply": "2025-10-13T14:18:44.249284Z"
    },
    "papermill": {
     "duration": 11.777942,
     "end_time": "2025-10-13T14:18:44.252100",
     "exception": false,
     "start_time": "2025-10-13T14:18:32.474158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.9/115.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -qq -r /kaggle/input/monodtr-library-ver2/MonoDTR/requirement.txt\n",
    "!pip install -qq coloredlogs\n",
    "!pip install -qq ptflops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0300c12f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:18:44.264233Z",
     "iopub.status.busy": "2025-10-13T14:18:44.263925Z",
     "iopub.status.idle": "2025-10-13T14:18:44.267533Z",
     "shell.execute_reply": "2025-10-13T14:18:44.266927Z"
    },
    "papermill": {
     "duration": 0.011006,
     "end_time": "2025-10-13T14:18:44.268880",
     "exception": false,
     "start_time": "2025-10-13T14:18:44.257874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"/kaggle/input/monodtr-library-ver2/MonoDTR/scripts\")\n",
    "sys.path.append(\"/kaggle/input/pretrain-monodtr-base\")\n",
    "sys.path.append(\"/kaggle/input/checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d321fcf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:18:44.280357Z",
     "iopub.status.busy": "2025-10-13T14:18:44.280126Z",
     "iopub.status.idle": "2025-10-13T14:18:47.552429Z",
     "shell.execute_reply": "2025-10-13T14:18:47.551508Z"
    },
    "papermill": {
     "duration": 3.279588,
     "end_time": "2025-10-13T14:18:47.553764",
     "exception": false,
     "start_time": "2025-10-13T14:18:44.274176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['deform_conv.py', 'deform_conv_ext.cpython-310-x86_64-linux-gnu.so', 'make.sh', 'src', '__init__.py', 'setup.py']\n",
      "Python version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]\n",
      "Import successful!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# Add the directory containing the .so file to the system path\n",
    "sys.path.append('/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/networks/lib/ops/dcn')\n",
    "\n",
    "# Check if the .so file exists\n",
    "print(os.listdir('/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/networks/lib/ops/dcn'))\n",
    "\n",
    "# Print Python version\n",
    "print(\"Python version:\", sys.version)\n",
    "\n",
    "# Set LD_LIBRARY_PATH\n",
    "os.environ['LD_LIBRARY_PATH'] = '/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/networks/lib/ops/dcn:' + os.environ.get('LD_LIBRARY_PATH', '')\n",
    "\n",
    "# Attempt to import the module\n",
    "try:\n",
    "    import deform_conv_ext\n",
    "    print(\"Import successful!\")\n",
    "except ImportError as e:\n",
    "    print(\"Import failed:\", e)\n",
    "\n",
    "# If needed, run the setup or make script\n",
    "# !python setup.py build\n",
    "# !python setup.py install\n",
    "# or\n",
    "# !bash make.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bea8b61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:18:47.565705Z",
     "iopub.status.busy": "2025-10-13T14:18:47.565368Z",
     "iopub.status.idle": "2025-10-13T14:18:47.633001Z",
     "shell.execute_reply": "2025-10-13T14:18:47.632130Z"
    },
    "papermill": {
     "duration": 0.074932,
     "end_time": "2025-10-13T14:18:47.634385",
     "exception": false,
     "start_time": "2025-10-13T14:18:47.559453",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!\n",
      "Number of GPUs available: 1\n",
      "Current GPU: 0\n",
      "GPU Name: Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available!\")\n",
    "    print(\"Number of GPUs available:\", torch.cuda.device_count())\n",
    "    print(\"Current GPU:\", torch.cuda.current_device())\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "else:\n",
    "    print(\"CUDA is not available. Check your settings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0aa90bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:18:47.646336Z",
     "iopub.status.busy": "2025-10-13T14:18:47.646079Z",
     "iopub.status.idle": "2025-10-13T14:18:47.822011Z",
     "shell.execute_reply": "2025-10-13T14:18:47.820934Z"
    },
    "papermill": {
     "duration": 0.183455,
     "end_time": "2025-10-13T14:18:47.823389",
     "exception": false,
     "start_time": "2025-10-13T14:18:47.639934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor created on GPU.\n",
      "Allocated GPU Memory: 400556032\n",
      "Cached GPU Memory: 400556032\n"
     ]
    }
   ],
   "source": [
    "# Create a large tensor and move it to the GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "x = torch.randn(10000, 10000, device=device)  # This should allocate GPU memory\n",
    "print(\"Tensor created on GPU.\")\n",
    "print(\"Allocated GPU Memory:\", torch.cuda.memory_allocated(device))\n",
    "print(\"Cached GPU Memory:\", torch.cuda.memory_reserved(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eae2c168",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:18:47.835545Z",
     "iopub.status.busy": "2025-10-13T14:18:47.835305Z",
     "iopub.status.idle": "2025-10-13T14:18:54.995231Z",
     "shell.execute_reply": "2025-10-13T14:18:54.994499Z"
    },
    "papermill": {
     "duration": 7.167486,
     "end_time": "2025-10-13T14:18:54.996749",
     "exception": false,
     "start_time": "2025-10-13T14:18:47.829263",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch\n",
    "from torch.nn import Module, Dropout\n",
    "from einops import rearrange\n",
    "import torch.nn.functional as F\n",
    "from ptflops import get_model_complexity_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7783365d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:18:55.008897Z",
     "iopub.status.busy": "2025-10-13T14:18:55.008658Z",
     "iopub.status.idle": "2025-10-13T14:18:55.013155Z",
     "shell.execute_reply": "2025-10-13T14:18:55.012526Z"
    },
    "papermill": {
     "duration": 0.011767,
     "end_time": "2025-10-13T14:18:55.014352",
     "exception": false,
     "start_time": "2025-10-13T14:18:55.002585",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -------------------------- RMSNorm --------------------------\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, L, D)\n",
    "        norm_x = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "        return self.scale * norm_x\n",
    "\n",
    "# -------------------------- FLIP Module --------------------------\n",
    "def flip(x, dim):\n",
    "    return torch.flip(x, dims=[dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4574cd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:18:55.025821Z",
     "iopub.status.busy": "2025-10-13T14:18:55.025617Z",
     "iopub.status.idle": "2025-10-13T14:18:55.033598Z",
     "shell.execute_reply": "2025-10-13T14:18:55.033001Z"
    },
    "papermill": {
     "duration": 0.015068,
     "end_time": "2025-10-13T14:18:55.034775",
     "exception": false,
     "start_time": "2025-10-13T14:18:55.019707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -------------------------- BiMamba2 Block (Conv Fusion) --------------------------\n",
    "class BiMamba2(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.linear_x = nn.Linear(dim, dim)\n",
    "        self.linear_bf = nn.Linear(dim, dim)\n",
    "        self.linear_bb = nn.Linear(dim, dim)\n",
    "        self.linear_z = nn.Linear(dim, dim)\n",
    "\n",
    "        self.concat_proj_f = nn.Linear(2 * dim, dim)\n",
    "        self.concat_proj_b = nn.Linear(2 * dim, dim)\n",
    "\n",
    "        self.conv1d_f = nn.Conv1d(dim, dim, kernel_size=1)\n",
    "        self.conv1d_b = nn.Conv1d(dim, dim, kernel_size=1)\n",
    "        self.conv1d_zf = nn.Conv1d(dim, dim, kernel_size=1)\n",
    "        self.conv1d_ab_z = nn.Conv1d(dim, dim, kernel_size=1)\n",
    "\n",
    "        self.ssms_f = nn.Sequential(\n",
    "            nn.Conv1d(dim, dim, 1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(dim, dim, 1)\n",
    "        )\n",
    "        self.ssms_b = nn.Sequential(\n",
    "            nn.Conv1d(dim, dim, 1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(dim, dim, 1)\n",
    "        )\n",
    "\n",
    "        self.norm_f = RMSNorm(dim)\n",
    "        self.norm_b = RMSNorm(dim)\n",
    "        self.output_linear = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, u):\n",
    "        # u: (B, L, D)\n",
    "        x = self.linear_x(u)          # (B, L, D)\n",
    "        bf = self.linear_bf(u)        # (B, L, D)\n",
    "        bb = self.linear_bb(u)        # (B, L, D)\n",
    "        z = self.linear_z(u)          # (B, L, D)\n",
    "\n",
    "        bf_x = torch.cat([bf, x], dim=-1)         # (B, L, 2D)\n",
    "        bb_x = torch.cat([bb, x], dim=-1)         # (B, L, 2D)\n",
    "\n",
    "        bf_x = self.concat_proj_f(bf_x)           # (B, L, D)\n",
    "        bb_x = self.concat_proj_b(bb_x)           # (B, L, D)\n",
    "\n",
    "        af = self.conv1d_f(bf_x.transpose(1, 2))  # (B, D, L)\n",
    "        af = F.gelu(af)     \n",
    "        af = self.ssms_f(af).transpose(1, 2)      # (B, L, D)# (B, D, L)\n",
    "        # af = af + self.conv1d_zf(z.transpose(1, 2))  # af conv with z\n",
    "        af = af * z\n",
    "        af = self.norm_f(af)                      # (B, L, D)\n",
    "\n",
    "        bb_x_flip = flip(bb_x, dim=1)             # (B, L, D)\n",
    "        ab = self.conv1d_b(bb_x_flip.transpose(1, 2))  # (B, D, L)\n",
    "        ab = F.gelu(ab)                                # (B, D, L)\n",
    "\n",
    "        z_flip = flip(z, dim=1)                        # (B, L, D)\n",
    "        ab = self.ssms_b(ab).transpose(1, 2)           # (B, L, D)\n",
    "        # ab = ab + self.conv1d_ab_z(z_flip.transpose(1, 2))  # (B, D, L)\n",
    "        ab = ab * z_flip   # (B, D, L)\n",
    "        ab = self.norm_b(ab)                           # (B, L, D)\n",
    "        ab = flip(ab, dim=1)                           # (B, L, D)\n",
    "\n",
    "        out = self.output_linear(af + ab)         # (B, L, D)                 # (B, L, D)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db378b00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:18:55.046191Z",
     "iopub.status.busy": "2025-10-13T14:18:55.045989Z",
     "iopub.status.idle": "2025-10-13T14:18:55.053826Z",
     "shell.execute_reply": "2025-10-13T14:18:55.053212Z"
    },
    "papermill": {
     "duration": 0.014915,
     "end_time": "2025-10-13T14:18:55.055057",
     "exception": false,
     "start_time": "2025-10-13T14:18:55.040142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -------------------------- CrossMamba2 Module --------------------------\n",
    "class CrossMamba2(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.linear_x = nn.Linear(dim, dim)\n",
    "        self.linear_z = nn.Linear(dim, dim)\n",
    "        self.linear_bf = nn.Linear(dim, dim)\n",
    "        self.linear_bb = nn.Linear(dim, dim)\n",
    "\n",
    "        self.concat_proj_f = nn.Linear(2 * dim, dim)\n",
    "        self.concat_proj_b = nn.Linear(2 * dim, dim)\n",
    "\n",
    "        self.conv1d_f = nn.Conv1d(dim, dim, kernel_size=1)\n",
    "        self.conv1d_b = nn.Conv1d(dim, dim, kernel_size=1)\n",
    "        self.conv1d_zf = nn.Conv1d(dim, dim, kernel_size=1)\n",
    "        self.conv1d_ab_z = nn.Conv1d(dim, dim, kernel_size=1)\n",
    "\n",
    "        self.ssms_f = nn.Sequential(\n",
    "            nn.Conv1d(dim, dim, 1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(dim, dim, 1)\n",
    "        )\n",
    "        self.ssms_b = nn.Sequential(\n",
    "            nn.Conv1d(dim, dim, 1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(dim, dim, 1)\n",
    "        )\n",
    "\n",
    "        self.norm_f = RMSNorm(dim)\n",
    "        self.norm_b = RMSNorm(dim)\n",
    "        self.output_linear = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, u1, u2):\n",
    "        # u1: context (B, L, D), u2: depth (B, L, D)\n",
    "        x = self.linear_x(u1)               # (B, L, D)\n",
    "        z = self.linear_z(u1)               # (B, L, D)\n",
    "        bf = self.linear_bf(u2)             # (B, L, D)\n",
    "        bb = self.linear_bb(u2)             # (B, L, D)\n",
    "\n",
    "        bf_x = torch.cat([bf, x], dim=-1)   # (B, L, 2D)\n",
    "        bb_x = torch.cat([bb, x], dim=-1)   # (B, L, 2D)\n",
    "\n",
    "        bf_x = self.concat_proj_f(bf_x)     # (B, L, D)\n",
    "        bb_x = self.concat_proj_b(bb_x)     # (B, L, D)\n",
    "\n",
    "        af = self.conv1d_f(bf_x.transpose(1, 2))  # (B, D, L)\n",
    "        af = F.gelu(af)\n",
    "        af = self.ssms_f(af).transpose(1, 2)      # (B, L, D)\n",
    "        # af = af + self.conv1d_zf(z.transpose(1, 2))  # af conv with z\n",
    "        af = af * z  # af conv with z\n",
    "        af = self.norm_f(af)\n",
    "\n",
    "        bb_x_flip = flip(bb_x, dim=1)             # (B, L, D)\n",
    "        ab = self.conv1d_b(bb_x_flip.transpose(1, 2))\n",
    "        ab = F.gelu(ab).transpose(1, 2)           # (B, L, D)\n",
    "\n",
    "        z_flip = flip(z, dim=1)\n",
    "        ab = self.ssms_b(ab.transpose(1, 2)).transpose(1, 2)\n",
    "        # ab = ab + self.conv1d_ab_z(z_flip.transpose(1, 2))  # (B, D, L)\n",
    "        ab = ab * z_flip  # (B, D, L)\n",
    "        ab = self.norm_b(ab)\n",
    "        ab = flip(ab, dim=1)\n",
    "\n",
    "        out = self.output_linear(af + ab)         # (B, L, D)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "623128f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:18:55.066528Z",
     "iopub.status.busy": "2025-10-13T14:18:55.066298Z",
     "iopub.status.idle": "2025-10-13T14:18:55.070185Z",
     "shell.execute_reply": "2025-10-13T14:18:55.069384Z"
    },
    "papermill": {
     "duration": 0.010965,
     "end_time": "2025-10-13T14:18:55.071359",
     "exception": false,
     "start_time": "2025-10-13T14:18:55.060394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -------------------------- FFN + Norm --------------------------\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81839a0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:18:55.082837Z",
     "iopub.status.busy": "2025-10-13T14:18:55.082605Z",
     "iopub.status.idle": "2025-10-13T14:18:55.088797Z",
     "shell.execute_reply": "2025-10-13T14:18:55.088015Z"
    },
    "papermill": {
     "duration": 0.013393,
     "end_time": "2025-10-13T14:18:55.090041",
     "exception": false,
     "start_time": "2025-10-13T14:18:55.076648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -------------------------- Depth Aware Mamba (DTR-Compatible Input) --------------------------\n",
    "class DepthAwareMamba(nn.Module):\n",
    "    def __init__(self, output_channel_num):\n",
    "        super().__init__()\n",
    "        self.output_channel_num = output_channel_num\n",
    "\n",
    "        self.encoder_bimamba = BiMamba2(self.output_channel_num)\n",
    "        self.encoder_ffn = FeedForward(self.output_channel_num, self.output_channel_num * 2)\n",
    "        self.encoder_norm1 = nn.LayerNorm(self.output_channel_num)\n",
    "        self.encoder_norm2 = nn.LayerNorm(self.output_channel_num)\n",
    "\n",
    "        self.decoder_bimamba = BiMamba2(self.output_channel_num)\n",
    "        self.cross_bimamba = CrossMamba2(self.output_channel_num)\n",
    "        self.decoder_ffn = FeedForward(self.output_channel_num, self.output_channel_num * 2)\n",
    "        self.decoder_norm1 = nn.LayerNorm(self.output_channel_num)\n",
    "        self.decoder_norm2 = nn.LayerNorm(self.output_channel_num)\n",
    "        self.decoder_norm3 = nn.LayerNorm(self.output_channel_num)\n",
    "\n",
    "    def forward(self, depth_feat, context_feat, depth_pos=None):\n",
    "        depth_feat = depth_feat.contiguous()\n",
    "        context_feat = context_feat.contiguous()\n",
    "        if depth_pos is not None:\n",
    "            depth_pos = depth_pos.contiguous()\n",
    "            context_feat = context_feat + depth_pos\n",
    "    \n",
    "        # Encoder on context_feat\n",
    "        x = self.encoder_bimamba(context_feat.contiguous())\n",
    "        x = self.encoder_norm1((x + context_feat).contiguous())\n",
    "        x_ffn = self.encoder_ffn(x.contiguous())\n",
    "        x = self.encoder_norm2((x + x_ffn).contiguous())\n",
    "    \n",
    "        # Decoder on depth_feat and fused context\n",
    "        d = self.decoder_bimamba(depth_feat.contiguous())\n",
    "        d = self.decoder_norm1((d + depth_feat).contiguous())\n",
    "    \n",
    "        x = self.cross_bimamba(x.contiguous(), d.contiguous())\n",
    "        x = self.decoder_norm2(x.contiguous())\n",
    "        x_ffn = self.decoder_ffn(x.contiguous())\n",
    "        x = self.decoder_norm3((x + x_ffn).contiguous())\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cafb6918",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:18:55.101438Z",
     "iopub.status.busy": "2025-10-13T14:18:55.101240Z",
     "iopub.status.idle": "2025-10-13T14:18:55.106781Z",
     "shell.execute_reply": "2025-10-13T14:18:55.106169Z"
    },
    "papermill": {
     "duration": 0.012534,
     "end_time": "2025-10-13T14:18:55.107974",
     "exception": false,
     "start_time": "2025-10-13T14:18:55.095440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CSA(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross-Scale Attention (CSA) module.\n",
    "    Projects both inputs to a common channel dimension 'c' before computing attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, c1, c2, c):\n",
    "        super(CSA, self).__init__()\n",
    "        self.proj_query = nn.Linear(c1, c)\n",
    "        self.proj_coarser = nn.Linear(c2, c)  # Shared for key and value\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        \"\"\"\n",
    "        :param x1: Finer-resolution input (B, C1, H1, W1)\n",
    "        :param x2: Coarser-resolution input (B, C2, H2, W2)\n",
    "        :return: Augmented finer-resolution feature (B, c, H1, W1)\n",
    "        \"\"\"\n",
    "        B, C1, H1, W1 = x1.shape\n",
    "        _, C2, H2, W2 = x2.shape\n",
    "\n",
    "        flat1 = x1.permute(0, 2, 3, 1).reshape(B, H1 * W1, C1)\n",
    "        flat2 = x2.permute(0, 2, 3, 1).reshape(B, H2 * W2, C2)\n",
    "\n",
    "        query = self.proj_query(flat1)\n",
    "        key = self.proj_coarser(flat2)\n",
    "        value = self.proj_coarser(flat2)\n",
    "\n",
    "        energy = torch.bmm(query, key.permute(0, 2, 1))\n",
    "        attention = F.softmax(energy, dim=-1)\n",
    "\n",
    "        out = torch.bmm(attention, value)\n",
    "\n",
    "        out = out + query\n",
    "        out = out.reshape(B, H1, W1, -1).permute(0, 3, 1, 2)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d4a33cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:18:55.119410Z",
     "iopub.status.busy": "2025-10-13T14:18:55.119187Z",
     "iopub.status.idle": "2025-10-13T14:18:55.124537Z",
     "shell.execute_reply": "2025-10-13T14:18:55.123747Z"
    },
    "papermill": {
     "duration": 0.012431,
     "end_time": "2025-10-13T14:18:55.125699",
     "exception": false,
     "start_time": "2025-10-13T14:18:55.113268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MSR(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Scale Refinement (MSR) module without depth head for feature refinement only.\n",
    "    \"\"\"\n",
    "    def __init__(self, d=256):\n",
    "        super(MSR, self).__init__()\n",
    "        self.mlp1 = nn.Sequential(nn.Conv2d(d, d, kernel_size=1), nn.ReLU(inplace=True))\n",
    "        self.mlp2 = nn.Sequential(nn.Conv2d(d, d, kernel_size=1), nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, coarser=None, finer=None):\n",
    "        if coarser is not None:\n",
    "            target_size = finer.shape[2:] if finer is not None else (coarser.shape[2] * 2, coarser.shape[3] * 2)\n",
    "            up = F.interpolate(coarser, size=target_size, mode='bilinear', align_corners=False)\n",
    "            if finer is not None:\n",
    "                feature = finer + up\n",
    "            else:\n",
    "                feature = up\n",
    "        else:\n",
    "            feature = finer\n",
    "        feature = self.mlp1(feature)\n",
    "        feature = self.mlp2(feature)\n",
    "        # CHANGE: Only return the refined feature (no depth)\n",
    "        return feature  # Was: return feature, depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbad7455",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:18:55.137077Z",
     "iopub.status.busy": "2025-10-13T14:18:55.136826Z",
     "iopub.status.idle": "2025-10-13T14:19:12.329773Z",
     "shell.execute_reply": "2025-10-13T14:19:12.329091Z"
    },
    "papermill": {
     "duration": 17.200195,
     "end_time": "2025-10-13T14:19:12.331300",
     "exception": false,
     "start_time": "2025-10-13T14:18:55.131105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from visualDet3D.networks.backbones.dla import dla102\n",
    "from visualDet3D.networks.backbones.dlaup import DLAUp\n",
    "from visualDet3D.networks.detectors.dfe import DepthAwareFE\n",
    "from visualDet3D.networks.detectors.dpe import DepthAwarePosEnc\n",
    "from visualDet3D.networks.detectors.dtr import DepthAwareTransformer\n",
    "import math\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df7398cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:19:12.344558Z",
     "iopub.status.busy": "2025-10-13T14:19:12.344052Z",
     "iopub.status.idle": "2025-10-13T14:19:12.353649Z",
     "shell.execute_reply": "2025-10-13T14:19:12.353063Z"
    },
    "papermill": {
     "duration": 0.017009,
     "end_time": "2025-10-13T14:19:12.354835",
     "exception": false,
     "start_time": "2025-10-13T14:19:12.337826",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "class MonoDTRCore(nn.Module):\n",
    "    def __init__(self, backbone_arguments=dict()):\n",
    "        super(MonoDTRCore, self).__init__()\n",
    "\n",
    "        # Swin-T backbone with features_only=True to ensure feature pyramid output\n",
    "        self.backbone = timm.create_model('swin_tiny_patch4_window7_224', pretrained=True,features_only=True, img_size=(288, 1280))\n",
    "        # self.backbone = timm.create_model('swin_tiny_patch4_window7_224', pretrained=True, features_only=True)\n",
    "        self.first_level = 1  # Kept for reference, but no longer used\n",
    "\n",
    "        # Feature pyramid channels from Swin-T\n",
    "        self.channels = [96, 192, 384, 768]\n",
    "        self.output_channel_num = 256\n",
    "\n",
    "        self.csa_1_8 = CSA(192, self.output_channel_num, self.output_channel_num)   # 1/8 with 1/16\n",
    "        self.csa_1_16 = CSA(384, 768, self.output_channel_num)  # 1/16 with 1/32\n",
    "\n",
    "        # MSR modules for upsampling and refinement\n",
    "        self.msr_levels = nn.ModuleList([MSR(self.output_channel_num) for _ in range(2)])\n",
    "        \n",
    "        self.dpe = DepthAwarePosEnc(self.output_channel_num)\n",
    "        self.depth_embed = nn.Embedding(100, self.output_channel_num)\n",
    "        self.dtr = DepthAwareTransformer(self.output_channel_num)\n",
    "        self.dfe = DepthAwareFE(self.output_channel_num)\n",
    "        self.img_conv = nn.Conv2d(self.output_channel_num, self.output_channel_num, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of MonoDTRCore.\n",
    "        \n",
    "        Input:\n",
    "            x: dict with key 'image' containing (N, 3, H, W)\n",
    "        \n",
    "        Output:\n",
    "            feat: (N, C, H/8, W/8) - Processed feature map for compatibility with dtr\n",
    "            depth: (N, D, H, W) - Depth prediction at 1/1 resolution\n",
    "        \"\"\"\n",
    "        if 'image' not in x:\n",
    "            raise ValueError(\"Input dictionary must contain 'image' key\")\n",
    "        img = x['image']  # (N, 3, H, W)\n",
    "        assert img.shape[2] == 288 and img.shape[3] == 1280, f\"Expected shape [N, 3, 288, 1280], got {img.shape}\"\n",
    "\n",
    "        if img.dim() != 4:\n",
    "            raise ValueError(f\"Expected 4D input tensor (N, C, H, W), got shape {img.shape}\")\n",
    "        \n",
    "        if img.shape[1] != 3:\n",
    "            raise ValueError(\n",
    "                f\"Expected input image to have 3 channels (RGB), got {img.shape[1]} channels. \"\n",
    "                \"Please check your data pipeline to ensure the input is correctly formatted as (N, 3, H, W). \"\n",
    "                \"If the input has 7 channels (e.g., multi-spectral), preprocess it to 3 channels or modify the backbone.\"\n",
    "            )\n",
    "\n",
    "        # Extract features using standard forward method\n",
    "        features = self.backbone(img)  # [(N, H/4, W/4, 96), (N, H/8, W/8, 192), (N, H/16, W/16, 384), (N, H/32, W/32, 768)]\n",
    "\n",
    "        # Permute features from (N, H, W, C) to (N, C, H, W)\n",
    "        features = [f.permute(0, 3, 1, 2) for f in features]  # [(N, 96, H/4, W/4), (N, 192, H/8, W/8), (N, 384, H/16, W/16), (N, 768, H/32, W/32)]\n",
    "\n",
    "\n",
    "        # Refine features progressively with CSA and MSR\n",
    "        fused = self.csa_1_16(features[2], features[3])  # (N, 256, H/16, W/16)\n",
    "        prev_feat = self.msr_levels[0](fused, None)\n",
    "\n",
    "        fused = self.csa_1_8(features[1], prev_feat)     # (N, 256, H/8, W/8)\n",
    "        prev_feat = self.msr_levels[1](fused, prev_feat)\n",
    "\n",
    "        x = prev_feat\n",
    "\n",
    "        # Proceed with downstream processing\n",
    "        N, C, H, W = x.shape  # H=H/8, W=W/8 (reduced resolution)\n",
    "        # print(f\"X shape: {x.shape}\")\n",
    "        depth, depth_guide, depth_feat = self.dfe(x) # depth: (N, D, H/8, W/8), depth_guide: (N, num_classes, H/8, W/8), depth_feat: (N, 256, H/8, W/8)\n",
    "        depth_feat = depth_feat.permute(0, 2, 3, 1).view(N, H * W, C)\n",
    "        depth_guide = depth_guide.argmax(1)\n",
    "        depth_emb = self.depth_embed(depth_guide).view(N, H * W, C)\n",
    "        depth_emb = self.dpe(depth_emb, (H, W))\n",
    "        img_feat = x + self.img_conv(x)\n",
    "        img_feat = img_feat.permute(0, 2, 3, 1)\n",
    "        # print(f\"img_feat shape: {img_feat.shape}\")\n",
    "        img_feat = img_feat.view(N, H * W, C)\n",
    "        feat = self.dtr(depth_feat, img_feat, depth_emb)\n",
    "        feat = feat.permute(0, 2, 1).view(N, C, H, W)\n",
    "\n",
    "        return feat, depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "199fca76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:19:12.366419Z",
     "iopub.status.busy": "2025-10-13T14:19:12.366206Z",
     "iopub.status.idle": "2025-10-13T14:19:12.369284Z",
     "shell.execute_reply": "2025-10-13T14:19:12.368667Z"
    },
    "papermill": {
     "duration": 0.010103,
     "end_time": "2025-10-13T14:19:12.370403",
     "exception": false,
     "start_time": "2025-10-13T14:19:12.360300",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from visualDet3D.networks.heads.detection_3d_head import AnchorBasedDetection3DHead\n",
    "from visualDet3D.networks.heads.depth_losses import bin_depths, DepthFocalLoss\n",
    "from visualDet3D.networks.utils.registry import DETECTOR_DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c712bf63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:19:12.381799Z",
     "iopub.status.busy": "2025-10-13T14:19:12.381571Z",
     "iopub.status.idle": "2025-10-13T14:19:12.389973Z",
     "shell.execute_reply": "2025-10-13T14:19:12.389337Z"
    },
    "papermill": {
     "duration": 0.015372,
     "end_time": "2025-10-13T14:19:12.391113",
     "exception": false,
     "start_time": "2025-10-13T14:19:12.375741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MonoDTR(nn.Module):\n",
    "    def __init__(self, network_cfg):\n",
    "        super(MonoDTR, self).__init__()\n",
    "\n",
    "        self.obj_types = network_cfg.obj_types\n",
    "\n",
    "        self.build_head(network_cfg)\n",
    "\n",
    "        self.build_core(network_cfg)\n",
    "\n",
    "        self.network_cfg = network_cfg\n",
    "\n",
    "    def build_core(self, network_cfg):\n",
    "        self.mono_core = MonoDTRCore(network_cfg.mono_backbone)\n",
    "\n",
    "    def build_head(self, network_cfg):\n",
    "        self.bbox_head = AnchorBasedDetection3DHead(\n",
    "            **(network_cfg.head)\n",
    "        )\n",
    "        self.depth_loss = DepthFocalLoss(96)\n",
    "\n",
    "    def train_forward(self, left_images, annotations, P2, depth_gt=None):\n",
    "        \n",
    "        features, depth = self.mono_core(dict(image=left_images, P2=P2))\n",
    "        \n",
    "        depth_output   = depth\n",
    "        \n",
    "        features = features.contiguous()\n",
    "        P2 = P2.contiguous()\n",
    "        left_images = left_images.contiguous()\n",
    "        \n",
    "        try:\n",
    "            cls_preds, reg_preds = self.bbox_head(\n",
    "                dict(\n",
    "                    features=features,\n",
    "                    P2=P2,\n",
    "                    image=left_images\n",
    "                )\n",
    "            )\n",
    "        except RuntimeError as e:\n",
    "            print(f\"RuntimeError: {e}\")\n",
    "            raise\n",
    "            \n",
    "        anchors = self.bbox_head.get_anchor(left_images, P2)\n",
    "\n",
    "        cls_loss, reg_loss, loss_dict = self.bbox_head.loss(cls_preds, reg_preds, anchors, annotations, P2)\n",
    "        \n",
    "        depth_gt = bin_depths(depth_gt, mode = \"LID\", depth_min=1, depth_max=80, num_bins=96, target=True)\n",
    "        \n",
    "        # if depth_gt is not None: #use in feature size is (H/4, W/4)\n",
    "        #         depth_gt = F.interpolate(depth_gt.unsqueeze(1), size=depth_output.shape[2:], mode='nearest').squeeze(1)\n",
    "            \n",
    "        if reg_loss.mean() > 0 and not depth_gt is None and not depth_output is None:\n",
    "            \n",
    "            depth_gt = depth_gt.unsqueeze(1)\n",
    "            depth_loss = 1.0 * self.depth_loss(depth_output, depth_gt)\n",
    "            loss_dict['depth_loss'] = depth_loss\n",
    "            reg_loss += depth_loss\n",
    "\n",
    "            self.depth_output = depth_output.detach()\n",
    "        else:\n",
    "            loss_dict['depth_loss'] = torch.zeros_like(reg_loss)\n",
    "        return cls_loss, reg_loss, loss_dict\n",
    "\n",
    "    def test_forward(self, left_images, P2):\n",
    "        assert left_images.shape[0] == 1 # we recommmend image batch size = 1 for testing\n",
    "\n",
    "        features, _ = self.mono_core(dict(image=left_images, P2=P2))\n",
    "        \n",
    "        cls_preds, reg_preds = self.bbox_head(\n",
    "                dict(\n",
    "                    features=features,\n",
    "                    P2=P2,\n",
    "                    image=left_images\n",
    "                )\n",
    "            )\n",
    "\n",
    "        anchors = self.bbox_head.get_anchor(left_images, P2)\n",
    "\n",
    "        scores, bboxes, cls_indexes = self.bbox_head.get_bboxes(cls_preds, reg_preds, anchors, P2, left_images)\n",
    "        \n",
    "        return scores, bboxes, cls_indexes\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        if isinstance(inputs, list) and len(inputs) >= 3:\n",
    "            return self.train_forward(*inputs)\n",
    "        else:\n",
    "            return self.test_forward(*inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4c674ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:19:12.402396Z",
     "iopub.status.busy": "2025-10-13T14:19:12.402184Z",
     "iopub.status.idle": "2025-10-13T14:19:12.452047Z",
     "shell.execute_reply": "2025-10-13T14:19:12.451421Z"
    },
    "papermill": {
     "duration": 0.056947,
     "end_time": "2025-10-13T14:19:12.453280",
     "exception": false,
     "start_time": "2025-10-13T14:19:12.396333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from easydict import EasyDict\n",
    "from tqdm import tqdm\n",
    "from fire import Fire\n",
    "import coloredlogs\n",
    "import logging\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from _path_init import *\n",
    "from visualDet3D.networks.utils.registry import DETECTOR_DICT, DATASET_DICT, PIPELINE_DICT\n",
    "from visualDet3D.networks.utils.utils import BackProjection, BBox3dProjector, get_num_parameters\n",
    "from visualDet3D.evaluator.kitti.evaluate import evaluate\n",
    "import visualDet3D.data.kitti.dataset\n",
    "from visualDet3D.utils.timer import Timer\n",
    "from visualDet3D.utils.utils import LossLogger, cfg_from_file\n",
    "from visualDet3D.networks.optimizers import optimizers, schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97f555ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:19:12.464712Z",
     "iopub.status.busy": "2025-10-13T14:19:12.464507Z",
     "iopub.status.idle": "2025-10-13T14:19:12.470612Z",
     "shell.execute_reply": "2025-10-13T14:19:12.470037Z"
    },
    "papermill": {
     "duration": 0.013206,
     "end_time": "2025-10-13T14:19:12.471860",
     "exception": false,
     "start_time": "2025-10-13T14:19:12.458654",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.cuda.amp import autocast  # For AMP\n",
    "import torch.optim as optim\n",
    "from visualDet3D.utils.utils import LossLogger\n",
    "from visualDet3D.utils.utils import compound_annotation\n",
    "\n",
    "def My_train_mono_detection(data, module: nn.Module,\n",
    "                         optimizer: optim.Optimizer,\n",
    "                         writer: SummaryWriter = None,\n",
    "                         loss_logger: LossLogger = None,\n",
    "                         global_step: int = None,\n",
    "                         epoch_num: int = None,\n",
    "                         cfg: EasyDict = EasyDict(),\n",
    "                         iter_num: int = None):\n",
    "    if iter_num is None:\n",
    "        iter_num = 0  # Fallback, but manage externally\n",
    "\n",
    "    images, P2, labels, bbox2d, bbox_3d, depth = data\n",
    "    # Assume batch=1, so lists of len=1\n",
    "\n",
    "    # Handle empty annotations gracefully\n",
    "    max_length = np.max([len(label) for label in labels]) if labels else 0\n",
    "    if max_length == 0:\n",
    "        return  # Still increments iter_num externally\n",
    "\n",
    "    annotation = compound_annotation(labels, max_length, bbox2d, bbox_3d, cfg.obj_types)\n",
    "    annotation_tensor = images.new(annotation).cuda()  # Inherit dtype/device\n",
    "\n",
    "    # Forward pass with AMP for efficiency\n",
    "    cls_loss, reg_loss, loss_dict = module([\n",
    "        images.cuda().float().contiguous(),\n",
    "        annotation_tensor,\n",
    "        P2.cuda(),\n",
    "        depth.cuda().contiguous()\n",
    "    ])\n",
    "    cls_loss = cls_loss.mean()\n",
    "    reg_loss = reg_loss.mean()\n",
    "    total_loss = cls_loss + reg_loss\n",
    "    # loss_dict['total_loss'] = total_loss.item()  # Add for logging\n",
    "\n",
    "    # Log unscaled micro-batch losses\n",
    "    if loss_logger is not None:\n",
    "        loss_logger.update(loss_dict)\n",
    "    del loss_dict\n",
    "        \n",
    "    if optimizer is not None:\n",
    "        # Check for invalid losses\n",
    "        if torch.isnan(total_loss) or torch.isinf(total_loss):\n",
    "            print(f\"Warning: Invalid loss at iter {iter_num}: {total_loss.item()}\")\n",
    "            total_loss = torch.tensor(0.0, device=total_loss.device, requires_grad=True)\n",
    "\n",
    "        total_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3af7b0f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:19:12.483255Z",
     "iopub.status.busy": "2025-10-13T14:19:12.483040Z",
     "iopub.status.idle": "2025-10-13T14:19:12.503898Z",
     "shell.execute_reply": "2025-10-13T14:19:12.503271Z"
    },
    "papermill": {
     "duration": 0.028041,
     "end_time": "2025-10-13T14:19:12.505151",
     "exception": false,
     "start_time": "2025-10-13T14:19:12.477110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict  # Add this import\n",
    "def main(config=\"/kaggle/input/monodtr-library-ver2/MonoDTR/config/config.py\", experiment_name=\"default\", world_size=1, local_rank=-1):\n",
    "    \"\"\"Main function for the training script.\n",
    "\n",
    "    KeywordArgs:\n",
    "        config (str): Path to config file.\n",
    "        experiment_name (str): Custom name for the experitment, only used in tensorboard.\n",
    "        world_size (int): Number of total subprocesses in distributed training. \n",
    "        local_rank: Rank of the process. Should not be manually assigned. 0-N for ranks in distributed training (only process 0 will print info and perform testing). -1 for single training. \n",
    "    \"\"\"\n",
    "\n",
    "    ## Get config\n",
    "    cfg = cfg_from_file(config)\n",
    "\n",
    "    ## Collect distributed(or not) information\n",
    "    cfg.dist = EasyDict()\n",
    "    cfg.dist.world_size = world_size\n",
    "    cfg.dist.local_rank = local_rank\n",
    "    is_distributed = local_rank >= 0 # local_rank < 0 -> single training\n",
    "    is_logging     = local_rank <= 0 # only log and test with main process\n",
    "    is_evaluating  = local_rank <= 0\n",
    "\n",
    "    ## Setup writer if local_rank > 0\n",
    "    recorder_dir = os.path.join(cfg.path.log_path, experiment_name + f\"config={config}\")\n",
    "    if is_logging: # writer exists only if not distributed and local rank is smaller\n",
    "        ## Clean up the dir if it exists before\n",
    "        if os.path.isdir(recorder_dir):\n",
    "            os.system(\"rm -r {}\".format(recorder_dir))\n",
    "            print(\"clean up the recorder directory of {}\".format(recorder_dir))\n",
    "        writer = SummaryWriter(recorder_dir)\n",
    "\n",
    "        ## Record config object using pprint\n",
    "        import pprint\n",
    "\n",
    "        formatted_cfg = pprint.pformat(cfg)\n",
    "        writer.add_text(\"config.py\", formatted_cfg.replace(' ', '&nbsp;').replace('\\n', '  \\n')) # add space for markdown style in tensorboard text\n",
    "    else:\n",
    "        writer = None\n",
    "\n",
    "    ## Set up GPU and distribution process\n",
    "    if is_distributed:\n",
    "        cfg.trainer.gpu = local_rank # local_rank will overwrite the GPU in configure file\n",
    "    gpu = min(cfg.trainer.gpu, torch.cuda.device_count() - 1)\n",
    "    torch.backends.cudnn.benchmark = getattr(cfg.trainer, 'cudnn', False)\n",
    "    torch.cuda.set_device(gpu)\n",
    "    if is_distributed:\n",
    "        torch.distributed.init_process_group(backend='nccl', init_method='env://')\n",
    "    print(local_rank)\n",
    " \n",
    "    ## define datasets and dataloader.\n",
    "    dataset_train = DATASET_DICT[cfg.data.train_dataset](cfg)\n",
    "    dataset_val = DATASET_DICT[cfg.data.val_dataset](cfg, \"validation\")\n",
    "\n",
    "    dataloader_train = DataLoader(dataset_train, num_workers=cfg.data.num_workers,\n",
    "                                  batch_size=cfg.data.batch_size, collate_fn=dataset_train.collate_fn, shuffle=local_rank<0, drop_last=True,\n",
    "                                  sampler=torch.utils.data.DistributedSampler(dataset_train, num_replicas=world_size, rank=local_rank, shuffle=True) if local_rank >= 0 else None)\n",
    "    dataloader_val = DataLoader(dataset_val, num_workers=cfg.data.num_workers,\n",
    "                                batch_size=cfg.data.batch_size, collate_fn=dataset_val.collate_fn, shuffle=False, drop_last=True)\n",
    "\n",
    "    detector = MonoDTR(cfg.detector)         \n",
    "            \n",
    "    ## Convert to cuda\n",
    "    if is_distributed:\n",
    "        detector = torch.nn.SyncBatchNorm.convert_sync_batchnorm(detector)\n",
    "        detector = torch.nn.parallel.DistributedDataParallel(detector.cuda(), device_ids=[gpu], output_device=gpu)\n",
    "    else:\n",
    "        detector = detector.cuda()\n",
    "    \n",
    "    ## define optimizer and weight decay\n",
    "    optimizer = optimizers.build_optimizer(cfg.optimizer, detector)\n",
    "\n",
    "    ## define scheduler\n",
    "    scheduler_config = getattr(cfg, 'scheduler', None)\n",
    "    scheduler = schedulers.build_scheduler(scheduler_config, optimizer)\n",
    "    is_iter_based = getattr(scheduler_config, \"is_iter_based\", False)\n",
    "\n",
    "    ## define loss logger\n",
    "    training_loss_logger =  LossLogger(writer, 'train') if is_logging else None\n",
    "\n",
    "    ## training pipeline\n",
    "    if 'training_func' in cfg.trainer:\n",
    "        # training_dection = PIPELINE_DICT[cfg.trainer.training_func] #hoanbi1\n",
    "        training_dection = My_train_mono_detection\n",
    "    else:\n",
    "        raise KeyError\n",
    "\n",
    "    ## Get evaluation pipeline\n",
    "    if 'evaluate_func' in cfg.trainer:\n",
    "        evaluate_detection = PIPELINE_DICT[cfg.trainer.evaluate_func]\n",
    "        print(\"Found evaluate function {}\".format(cfg.trainer.evaluate_func))\n",
    "    else:\n",
    "        evaluate_detection = None\n",
    "        print(\"Evaluate function not found\")\n",
    "\n",
    "\n",
    "    ## timer is used to estimate eta\n",
    "    timer = Timer()\n",
    "\n",
    "    print('Num training images: {}'.format(len(dataset_train)))\n",
    "\n",
    "    resume = True\n",
    "    if resume == True:\n",
    "        ckpt_path = os.path.join(\"/kaggle/input/monodtr-resume-checkpoint\", \"checkpoint_resume.pt\")\n",
    "        dict_checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "        detector.load_state_dict(dict_checkpoint[\"state_dict_backbone\"])\n",
    "        optimizer.load_state_dict(dict_checkpoint[\"state_optimizer\"])\n",
    "        # scheduler.load_state_dict(dict_checkpoint[\"state_lr_scheduler\"])\n",
    "        epoch = dict_checkpoint[\"epoch\"]\n",
    "        print(f\"Resuming training at epoch {epoch}\")\n",
    "    else:\n",
    "        epoch = 0\n",
    "        print(\"Start training at epoch 0\")\n",
    "\n",
    "    # this is for load pretrain checkpoint\n",
    "    state_dict_pretrain = torch.load(\"/kaggle/input/pretrain-monodtr-base/MonoDTR.pth\", map_location='cpu')\n",
    "    \n",
    "    # Filter out incompatible keys\n",
    "    new_state_dict = OrderedDict()\n",
    "    loaded_keys = []  # Track which parameters are loaded\n",
    "    for k, v in state_dict_pretrain.items():\n",
    "        if k in detector.state_dict() and v.shape == detector.state_dict()[k].shape:\n",
    "            new_state_dict[k] = v\n",
    "            loaded_keys.append(k)  # Record loaded keys\n",
    "            print(f\"Parameter {k} is loaded from pretrain\")\n",
    "\n",
    "    detector.load_state_dict(new_state_dict, strict=False)\n",
    "    \n",
    "    # Freeze specific layers in the Swin-T backbone (first two stages)\n",
    "    for name, param in detector.named_parameters():\n",
    "        # print(name)\n",
    "        if name in loaded_keys:\n",
    "            param.requires_grad = False  # Freeze loaded weights\n",
    "        else:\n",
    "            param.requires_grad = True\n",
    "\n",
    "    for name, param in detector.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(f\"Parameter {name} is trainable\")\n",
    "            \n",
    "    ## Record basic information of the model\n",
    "    if is_logging:\n",
    "        string1 = detector.__str__().replace(' ', '&nbsp;').replace('\\n', '  \\n')\n",
    "        writer.add_text(\"model structure\", string1) # add space for markdown style in tensorboard text\n",
    "        num_parameters = get_num_parameters(detector)\n",
    "        print(f'number of trained parameters of the model: {num_parameters}')\n",
    "        \n",
    "    detector.train()\n",
    "    global_step = 0\n",
    "    \n",
    "    # for epoch_num in range(cfg.trainer.max_epochs):\n",
    "    for epoch_num in range(epoch, (60+epoch)):\n",
    "        ## Start training for one epoch\n",
    "        torch.cuda.empty_cache()\n",
    "        detector.train()\n",
    "        \n",
    "        if training_loss_logger:\n",
    "            training_loss_logger.reset()\n",
    "\n",
    "        if optimizer is not None:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "        for iter_num, data in enumerate(dataloader_train):\n",
    "            training_dection(data, detector, optimizer, writer, training_loss_logger, global_step, epoch_num, cfg, iter_num)\n",
    "            global_step += 1\n",
    "            if is_logging and global_step % cfg.trainer.disp_iter == 0:\n",
    "                ## Log loss, print out and write to tensorboard in main process\n",
    "                if 'total_loss' not in training_loss_logger.loss_stats:\n",
    "                    print(f\"\\nIn epoch {epoch_num}, iteration:{iter_num}, global_step:{global_step}, total_loss not found in logger.\")\n",
    "                else:\n",
    "                    log_str = 'Epoch: {} | Iteration: {}  | Running loss: {:1.5f} | eta:{}'.format(\n",
    "                        epoch_num, iter_num, training_loss_logger.loss_stats['total_loss'].avg,\n",
    "                        timer.compute_eta(global_step, len(dataloader_train) * cfg.trainer.max_epochs))\n",
    "                    print(log_str, end='\\r')\n",
    "                    \n",
    "        if not is_iter_based:\n",
    "            num_iters = len(dataloader_train)\n",
    "            for _ in range(num_iters):\n",
    "                scheduler.step()\n",
    "        else:\n",
    "            scheduler.step()\n",
    "            \n",
    "        # Gradient clipping (use detector.parameters() to support both DDP and single-GPU)\n",
    "        if hasattr(cfg.optimizer, 'clipped_gradient_norm'):\n",
    "            torch.nn.utils.clip_grad_norm_(detector.parameters(), cfg.optimizer.clipped_gradient_norm)\n",
    "        else:\n",
    "            print(\"Warning: No clipped_gradient_norm in cfg; skipping clipping.\")\n",
    "            \n",
    "        # Single optimizer.step() here (update weights once per epoch)\n",
    "        if optimizer is not None:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "        # --- LOGGING: TensorBoard & loss logger per-epoch (instead of per-iteration) ---\n",
    "        if is_logging:\n",
    "            if 'total_loss' not in training_loss_logger.loss_stats:\n",
    "                print(f\"\\nIn epoch {epoch_num}, total_loss not found in logger.\")\n",
    "            else:\n",
    "                log_str = 'Epoch: {} | Running loss: {:1.5f} | eta:{}'.format(\n",
    "                    epoch_num, training_loss_logger.loss_stats['total_loss'].avg,\n",
    "                    timer.compute_eta(global_step, len(dataloader_train) * cfg.trainer.max_epochs))\n",
    "                print(log_str)\n",
    "                # write per-epoch (use epoch_num as step)\n",
    "                writer.add_text(\"training_log/train\", log_str, epoch_num)\n",
    "                # If you want scalars, you can add e.g.:\n",
    "                writer.add_scalar(\"train/total_loss\", training_loss_logger.loss_stats['total_loss'].avg, epoch_num)\n",
    "                # and log the rest of loss stats if present\n",
    "                training_loss_logger.log(epoch_num)\n",
    "                \n",
    "        ## save model in main process if needed\n",
    "        if is_logging:\n",
    "            torch.save(detector.module.state_dict() if is_distributed else detector.state_dict(), os.path.join(\n",
    "                cfg.path.checkpoint_path, '{}_latest.pth'.format(\n",
    "                    cfg.detector.name)\n",
    "                )\n",
    "            )\n",
    "        if is_logging and (epoch_num + 1) % cfg.trainer.save_iter == 0:\n",
    "            torch.save(detector.module.state_dict() if is_distributed else detector.state_dict(), os.path.join(\n",
    "                cfg.path.checkpoint_path, '{}_{}.pth'.format(\n",
    "                    cfg.detector.name,epoch_num)\n",
    "                )\n",
    "            )\n",
    "        checkpoint = {\n",
    "                \"epoch\": epoch_num + 1,\n",
    "                \"global_step\": global_step,\n",
    "                \"state_dict_backbone\": detector.state_dict(),\n",
    "                \"state_optimizer\": optimizer.state_dict(),\n",
    "                \"state_lr_scheduler\": scheduler.state_dict()\n",
    "            }\n",
    "        torch.save(checkpoint, os.path.join(cfg.path.checkpoint_path, f\"checkpoint_resume.pt\"))\n",
    "        print(f\"Save checkpoint at epoch {epoch_num+1} successfully!\")\n",
    "        ## test model in main process if needed\n",
    "        if is_evaluating and evaluate_detection is not None and cfg.trainer.test_iter > 0 and (epoch_num + 1) % cfg.trainer.test_iter == 0:\n",
    "            print(\"\\n/**** start testing after training epoch {} ******/\".format(epoch_num))\n",
    "            evaluate_detection(cfg, detector.module if is_distributed else detector, dataset_val, writer, epoch_num)\n",
    "            print(\"/**** finish testing after training epoch {} ******/\".format(epoch_num))\n",
    "\n",
    "        if is_distributed:\n",
    "            torch.distributed.barrier() # wait untill all finish a epoch\n",
    "\n",
    "        if is_logging:\n",
    "            writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73719191",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:19:12.516652Z",
     "iopub.status.busy": "2025-10-13T14:19:12.516447Z",
     "iopub.status.idle": "2025-10-13T14:19:12.522123Z",
     "shell.execute_reply": "2025-10-13T14:19:12.521322Z"
    },
    "papermill": {
     "duration": 0.012874,
     "end_time": "2025-10-13T14:19:12.523382",
     "exception": false,
     "start_time": "2025-10-13T14:19:12.510508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing my_config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile my_config.py\n",
    "\n",
    "from easydict import EasyDict as edict\n",
    "import os \n",
    "import numpy as np\n",
    "\n",
    "cfg = edict()\n",
    "cfg.obj_types = ['Car']\n",
    "#cfg.obj_types = ['Car', 'Pedestrian', 'Cyclist']\n",
    "\n",
    "## trainer\n",
    "trainer = edict(\n",
    "    gpu = 0,\n",
    "    max_epochs = 240, \n",
    "    disp_iter = 100,\n",
    "    save_iter = 5,\n",
    "    test_iter = 10,\n",
    "    training_func = \"train_mono_detection\",\n",
    "    test_func = \"test_mono_detection\",\n",
    "    evaluate_func = \"evaluate_kitti_obj\",\n",
    "    accumulation_steps = 1,  # hoanbi1\n",
    ")\n",
    "\n",
    "cfg.trainer = trainer\n",
    "\n",
    "## path\n",
    "path = edict()\n",
    "path.data_path = \"/kaggle/input/kitti-3d-object-detection-dataset/training\" # used in visualDet3D/data/.../dataset\n",
    "path.test_path = \"/kaggle/input/kitti-3d-object-detection-dataset/testing\" # used in visualDet3D/data/.../dataset\n",
    "path.visualDet3D_path = \"/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D\" # The path should point to the inner subfolder\n",
    "path.project_path = \"/kaggle/working/\" # or other path for pickle files, checkpoints, tensorboard logging and output files.\n",
    "if not os.path.isdir(path.project_path):\n",
    "    os.mkdir(path.project_path)\n",
    "path.project_path = os.path.join(path.project_path, 'MonoDTR')\n",
    "if not os.path.isdir(path.project_path):\n",
    "    os.mkdir(path.project_path)\n",
    "\n",
    "path.log_path = os.path.join(path.project_path, \"log\")\n",
    "if not os.path.isdir(path.log_path):\n",
    "    os.mkdir(path.log_path)\n",
    "\n",
    "path.checkpoint_path = os.path.join(path.project_path, \"checkpoint\")\n",
    "if not os.path.isdir(path.checkpoint_path):\n",
    "    os.mkdir(path.checkpoint_path)\n",
    "\n",
    "path.preprocessed_path = os.path.join(path.project_path, \"output\")\n",
    "if not os.path.isdir(path.preprocessed_path):\n",
    "    os.mkdir(path.preprocessed_path)\n",
    "\n",
    "path.train_imdb_path = os.path.join(path.preprocessed_path, \"training\")\n",
    "if not os.path.isdir(path.train_imdb_path):\n",
    "    os.mkdir(path.train_imdb_path)\n",
    "\n",
    "path.val_imdb_path = os.path.join(path.preprocessed_path, \"validation\")\n",
    "if not os.path.isdir(path.val_imdb_path):\n",
    "    os.mkdir(path.val_imdb_path)\n",
    "\n",
    "path.test_imdb_path = os.path.join(path.preprocessed_path, \"test\")\n",
    "if not os.path.isdir(path.test_imdb_path):\n",
    "    os.mkdir(path.test_imdb_path)\n",
    "\n",
    "path.test_result_path = os.path.join(path.test_imdb_path, \"data\")\n",
    "if not os.path.isdir(path.test_result_path):\n",
    "    os.mkdir(path.test_result_path)\n",
    "\n",
    "cfg.path = path\n",
    "\n",
    "## optimizer\n",
    "optimizer = edict(\n",
    "    type_name = 'adam',\n",
    "    keywords = edict(\n",
    "        lr        = 1e-4,\n",
    "        weight_decay = 0,\n",
    "    ),\n",
    "    clipped_gradient_norm = 0.1\n",
    ")\n",
    "cfg.optimizer = optimizer\n",
    "## scheduler\n",
    "scheduler = edict(\n",
    "    type_name = 'CosineAnnealingLR',\n",
    "    keywords = edict(\n",
    "        T_max     = cfg.trainer.max_epochs,\n",
    "        eta_min   = 5e-6,\n",
    "    )\n",
    ")\n",
    "cfg.scheduler = scheduler\n",
    "\n",
    "## data\n",
    "data = edict(\n",
    "    batch_size = 8,\n",
    "    num_workers = 8,\n",
    "    rgb_shape = (288, 1280, 3),\n",
    "    train_dataset = \"KittiMonoDataset\",\n",
    "    val_dataset   = \"KittiMonoDataset\",\n",
    "    test_dataset  = \"KittiMonoTestDataset\",\n",
    "    train_split_file = os.path.join(cfg.path.visualDet3D_path, 'data', 'kitti', 'chen_split', 'train.txt'),\n",
    "    val_split_file   = os.path.join(cfg.path.visualDet3D_path, 'data', 'kitti', 'chen_split', 'val.txt'),\n",
    ")\n",
    "\n",
    "data.augmentation = edict(\n",
    "    rgb_mean = np.array([0.485, 0.456, 0.406]),\n",
    "    rgb_std  = np.array([0.229, 0.224, 0.225]),\n",
    "    cropSize = (data.rgb_shape[0], data.rgb_shape[1]),\n",
    "    crop_top = 100,\n",
    ")\n",
    "data.train_augmentation = [\n",
    "    edict(type_name='ConvertToFloat'),\n",
    "    edict(type_name='PhotometricDistort', keywords=edict(distort_prob=1.0, contrast_lower=0.5, contrast_upper=1.5, saturation_lower=0.5, saturation_upper=1.5, hue_delta=18.0, brightness_delta=32)),\n",
    "    edict(type_name='CropTop', keywords=edict(crop_top_index=data.augmentation.crop_top)),\n",
    "    edict(type_name='Resize', keywords=edict(size=data.augmentation.cropSize)),\n",
    "    edict(type_name='RandomMirror', keywords=edict(mirror_prob=0.5)),\n",
    "    edict(type_name='Normalize', keywords=edict(mean=data.augmentation.rgb_mean, stds=data.augmentation.rgb_std))\n",
    "]\n",
    "data.test_augmentation = [\n",
    "    edict(type_name='ConvertToFloat'),\n",
    "    edict(type_name='CropTop', keywords=edict(crop_top_index=data.augmentation.crop_top)),\n",
    "    edict(type_name='Resize', keywords=edict(size=data.augmentation.cropSize)),\n",
    "    edict(type_name='Normalize', keywords=edict(mean=data.augmentation.rgb_mean, stds=data.augmentation.rgb_std))\n",
    "]\n",
    "cfg.data = data\n",
    "\n",
    "## networks\n",
    "detector = edict()\n",
    "detector.obj_types = cfg.obj_types\n",
    "detector.name = 'MonoDTR'\n",
    "detector.mono_backbone=edict(\n",
    ")\n",
    "head_loss = edict(\n",
    "    fg_iou_threshold = 0.5,\n",
    "    bg_iou_threshold = 0.4,\n",
    "    L1_regression_alpha = 5 ** 2,\n",
    "    focal_loss_gamma = 2.0,\n",
    "    balance_weight   = [20.0],\n",
    "    #balance_weight   = [20.0, 40, 40],\n",
    "    regression_weight = [1, 1, 1, 1, 1, 1, 12, 1, 1, 0.5, 0.5, 0.5, 1], #[x, y, w, h, cx, cy, z, sin2a, cos2a, w, h, l]\n",
    ")\n",
    "head_test = edict(\n",
    "    score_thr=0.75,\n",
    "    cls_agnostic = False,\n",
    "    nms_iou_thr=0.4,\n",
    "    post_optimization=False\n",
    ")\n",
    "\n",
    "anchors = edict(\n",
    "        {\n",
    "            'obj_types': cfg.obj_types,\n",
    "            'pyramid_levels':[3],\n",
    "            'strides': [2 ** 3],\n",
    "            'sizes' : [24],\n",
    "            'ratios': np.array([0.5, 1, 2.0]),\n",
    "            'scales': np.array([2 ** (i / 4.0) for i in range(16)]),\n",
    "        }\n",
    "    )\n",
    "\n",
    "head_layer = edict(\n",
    "    num_features_in=256,\n",
    "    num_cls_output=len(cfg.obj_types)+1,\n",
    "    num_reg_output=12,\n",
    "    cls_feature_size=256,\n",
    "    reg_feature_size=256,\n",
    ")\n",
    "detector.head = edict(\n",
    "    num_regression_loss_terms=13,\n",
    "    preprocessed_path=path.preprocessed_path,\n",
    "    num_classes     = len(cfg.obj_types),\n",
    "    anchors_cfg     = anchors,\n",
    "    layer_cfg       = head_layer,\n",
    "    loss_cfg        = head_loss,\n",
    "    test_cfg        = head_test\n",
    ")\n",
    "detector.anchors = anchors\n",
    "detector.loss = head_loss\n",
    "cfg.detector = detector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4295f160",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:19:12.534624Z",
     "iopub.status.busy": "2025-10-13T14:19:12.534393Z",
     "iopub.status.idle": "2025-10-13T14:19:12.538462Z",
     "shell.execute_reply": "2025-10-13T14:19:12.537685Z"
    },
    "papermill": {
     "duration": 0.010852,
     "end_time": "2025-10-13T14:19:12.539599",
     "exception": false,
     "start_time": "2025-10-13T14:19:12.528747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_training():\n",
    "    # config_path = \"/kaggle/input/monodtr-library-ver2/MonoDTR/config/config.py\"  # Path to your config file\n",
    "    config_path = \"/kaggle/working/my_config.py\"  # Path to your config file\n",
    "    experiment_name = \"EXP_NAME\"  # Use the defined experiment name\n",
    "    world_size = 1  # For single GPU training\n",
    "    local_rank = 0  # Local rank set to 0 as per your command\n",
    "    %env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
    "    main(config=config_path, experiment_name=experiment_name, world_size=world_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "058952a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T14:19:12.550946Z",
     "iopub.status.busy": "2025-10-13T14:19:12.550736Z",
     "iopub.status.idle": "2025-10-13T22:58:49.678283Z",
     "shell.execute_reply": "2025-10-13T22:58:49.677138Z"
    },
    "papermill": {
     "duration": 31177.135629,
     "end_time": "2025-10-13T22:58:49.680653",
     "exception": false,
     "start_time": "2025-10-13T14:19:12.545024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
      "-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25178a70168d474cbb1453785d6c14f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/114M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found evaluate function evaluate_kitti_obj\n",
      "Num training images: 3712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-49bdeb70fc8c>:104: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dict_checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training at epoch 240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-49bdeb70fc8c>:115: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict_pretrain = torch.load(\"/kaggle/input/pretrain-monodtr-base/MonoDTR.pth\", map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter bbox_head.balance_weights is loaded from pretrain\n",
      "Parameter bbox_head.regression_weight is loaded from pretrain\n",
      "Parameter bbox_head.loss_cls.balance_weights is loaded from pretrain\n",
      "Parameter bbox_head.cls_feature_extraction.0.weight is loaded from pretrain\n",
      "Parameter bbox_head.cls_feature_extraction.0.bias is loaded from pretrain\n",
      "Parameter bbox_head.cls_feature_extraction.3.weight is loaded from pretrain\n",
      "Parameter bbox_head.cls_feature_extraction.3.bias is loaded from pretrain\n",
      "Parameter bbox_head.cls_feature_extraction.6.weight is loaded from pretrain\n",
      "Parameter bbox_head.cls_feature_extraction.6.bias is loaded from pretrain\n",
      "Parameter bbox_head.reg_feature_extraction.0.weight is loaded from pretrain\n",
      "Parameter bbox_head.reg_feature_extraction.0.bias is loaded from pretrain\n",
      "Parameter bbox_head.reg_feature_extraction.0.conv_offset.weight is loaded from pretrain\n",
      "Parameter bbox_head.reg_feature_extraction.0.conv_offset.bias is loaded from pretrain\n",
      "Parameter bbox_head.reg_feature_extraction.1.weight is loaded from pretrain\n",
      "Parameter bbox_head.reg_feature_extraction.1.bias is loaded from pretrain\n",
      "Parameter bbox_head.reg_feature_extraction.1.running_mean is loaded from pretrain\n",
      "Parameter bbox_head.reg_feature_extraction.1.running_var is loaded from pretrain\n",
      "Parameter bbox_head.reg_feature_extraction.1.num_batches_tracked is loaded from pretrain\n",
      "Parameter bbox_head.reg_feature_extraction.3.weight is loaded from pretrain\n",
      "Parameter bbox_head.reg_feature_extraction.3.bias is loaded from pretrain\n",
      "Parameter bbox_head.reg_feature_extraction.4.weight is loaded from pretrain\n",
      "Parameter bbox_head.reg_feature_extraction.4.bias is loaded from pretrain\n",
      "Parameter bbox_head.reg_feature_extraction.4.running_mean is loaded from pretrain\n",
      "Parameter bbox_head.reg_feature_extraction.4.running_var is loaded from pretrain\n",
      "Parameter bbox_head.reg_feature_extraction.4.num_batches_tracked is loaded from pretrain\n",
      "Parameter bbox_head.reg_feature_extraction.6.weight is loaded from pretrain\n",
      "Parameter bbox_head.reg_feature_extraction.6.bias is loaded from pretrain\n",
      "Parameter mono_core.dpe.proj.weight is loaded from pretrain\n",
      "Parameter mono_core.dpe.proj.bias is loaded from pretrain\n",
      "Parameter mono_core.depth_embed.weight is loaded from pretrain\n",
      "Parameter mono_core.dtr.encoder.q_proj.weight is loaded from pretrain\n",
      "Parameter mono_core.dtr.encoder.k_proj.weight is loaded from pretrain\n",
      "Parameter mono_core.dtr.encoder.v_proj.weight is loaded from pretrain\n",
      "Parameter mono_core.dtr.encoder.merge.weight is loaded from pretrain\n",
      "Parameter mono_core.dtr.encoder.mlp.0.weight is loaded from pretrain\n",
      "Parameter mono_core.dtr.encoder.mlp.2.weight is loaded from pretrain\n",
      "Parameter mono_core.dtr.encoder.norm1.weight is loaded from pretrain\n",
      "Parameter mono_core.dtr.encoder.norm1.bias is loaded from pretrain\n",
      "Parameter mono_core.dtr.encoder.norm2.weight is loaded from pretrain\n",
      "Parameter mono_core.dtr.encoder.norm2.bias is loaded from pretrain\n",
      "Parameter mono_core.dtr.decoder.q_proj0.weight is loaded from pretrain\n",
      "Parameter mono_core.dtr.decoder.k_proj0.weight is loaded from pretrain\n",
      "Parameter mono_core.dtr.decoder.v_proj0.weight is loaded from pretrain\n",
      "Parameter mono_core.dtr.decoder.merge0.weight is loaded from pretrain\n",
      "Parameter mono_core.dtr.decoder.q_proj1.weight is loaded from pretrain\n",
      "Parameter mono_core.dtr.decoder.k_proj1.weight is loaded from pretrain\n",
      "Parameter mono_core.dtr.decoder.v_proj1.weight is loaded from pretrain\n",
      "Parameter mono_core.dtr.decoder.merge1.weight is loaded from pretrain\n",
      "Parameter mono_core.dtr.decoder.mlp.0.weight is loaded from pretrain\n",
      "Parameter mono_core.dtr.decoder.mlp.2.weight is loaded from pretrain\n",
      "Parameter mono_core.dtr.decoder.norm0.weight is loaded from pretrain\n",
      "Parameter mono_core.dtr.decoder.norm0.bias is loaded from pretrain\n",
      "Parameter mono_core.dtr.decoder.norm1.weight is loaded from pretrain\n",
      "Parameter mono_core.dtr.decoder.norm1.bias is loaded from pretrain\n",
      "Parameter mono_core.dtr.decoder.norm2.weight is loaded from pretrain\n",
      "Parameter mono_core.dtr.decoder.norm2.bias is loaded from pretrain\n",
      "Parameter mono_core.dfe.depth_output.1.weight is loaded from pretrain\n",
      "Parameter mono_core.dfe.depth_output.1.bias is loaded from pretrain\n",
      "Parameter mono_core.dfe.depth_output.2.weight is loaded from pretrain\n",
      "Parameter mono_core.dfe.depth_output.2.bias is loaded from pretrain\n",
      "Parameter mono_core.dfe.depth_output.2.running_mean is loaded from pretrain\n",
      "Parameter mono_core.dfe.depth_output.2.running_var is loaded from pretrain\n",
      "Parameter mono_core.dfe.depth_output.2.num_batches_tracked is loaded from pretrain\n",
      "Parameter mono_core.dfe.depth_output.4.weight is loaded from pretrain\n",
      "Parameter mono_core.dfe.depth_output.4.bias is loaded from pretrain\n",
      "Parameter mono_core.dfe.depth_down.weight is loaded from pretrain\n",
      "Parameter mono_core.dfe.depth_down.bias is loaded from pretrain\n",
      "Parameter mono_core.dfe.acf.conv1.0.weight is loaded from pretrain\n",
      "Parameter mono_core.dfe.acf.conv1.1.weight is loaded from pretrain\n",
      "Parameter mono_core.dfe.acf.conv1.1.bias is loaded from pretrain\n",
      "Parameter mono_core.dfe.acf.conv1.1.running_mean is loaded from pretrain\n",
      "Parameter mono_core.dfe.acf.conv1.1.running_var is loaded from pretrain\n",
      "Parameter mono_core.dfe.acf.conv1.1.num_batches_tracked is loaded from pretrain\n",
      "Parameter mono_core.dfe.acf.conv2.weight is loaded from pretrain\n",
      "Parameter mono_core.dfe.acf.conv2.bias is loaded from pretrain\n",
      "Parameter mono_core.img_conv.weight is loaded from pretrain\n",
      "Parameter mono_core.img_conv.bias is loaded from pretrain\n",
      "Parameter mono_core.backbone.patch_embed.proj.weight is trainable\n",
      "Parameter mono_core.backbone.patch_embed.proj.bias is trainable\n",
      "Parameter mono_core.backbone.patch_embed.norm.weight is trainable\n",
      "Parameter mono_core.backbone.patch_embed.norm.bias is trainable\n",
      "Parameter mono_core.backbone.layers_0.blocks.0.norm1.weight is trainable\n",
      "Parameter mono_core.backbone.layers_0.blocks.0.norm1.bias is trainable\n",
      "Parameter mono_core.backbone.layers_0.blocks.0.attn.relative_position_bias_table is trainable\n",
      "Parameter mono_core.backbone.layers_0.blocks.0.attn.qkv.weight is trainable\n",
      "Parameter mono_core.backbone.layers_0.blocks.0.attn.qkv.bias is trainable\n",
      "Parameter mono_core.backbone.layers_0.blocks.0.attn.proj.weight is trainable\n",
      "Parameter mono_core.backbone.layers_0.blocks.0.attn.proj.bias is trainable\n",
      "Parameter mono_core.backbone.layers_0.blocks.0.norm2.weight is trainable\n",
      "Parameter mono_core.backbone.layers_0.blocks.0.norm2.bias is trainable\n",
      "Parameter mono_core.backbone.layers_0.blocks.0.mlp.fc1.weight is trainable\n",
      "Parameter mono_core.backbone.layers_0.blocks.0.mlp.fc1.bias is trainable\n",
      "Parameter mono_core.backbone.layers_0.blocks.0.mlp.fc2.weight is trainable\n",
      "Parameter mono_core.backbone.layers_0.blocks.0.mlp.fc2.bias is trainable\n",
      "Parameter mono_core.backbone.layers_0.blocks.1.norm1.weight is trainable\n",
      "Parameter mono_core.backbone.layers_0.blocks.1.norm1.bias is trainable\n",
      "Parameter mono_core.backbone.layers_0.blocks.1.attn.relative_position_bias_table is trainable\n",
      "Parameter mono_core.backbone.layers_0.blocks.1.attn.qkv.weight is trainable\n",
      "Parameter mono_core.backbone.layers_0.blocks.1.attn.qkv.bias is trainable\n",
      "Parameter mono_core.backbone.layers_0.blocks.1.attn.proj.weight is trainable\n",
      "Parameter mono_core.backbone.layers_0.blocks.1.attn.proj.bias is trainable\n",
      "Parameter mono_core.backbone.layers_0.blocks.1.norm2.weight is trainable\n",
      "Parameter mono_core.backbone.layers_0.blocks.1.norm2.bias is trainable\n",
      "Parameter mono_core.backbone.layers_0.blocks.1.mlp.fc1.weight is trainable\n",
      "Parameter mono_core.backbone.layers_0.blocks.1.mlp.fc1.bias is trainable\n",
      "Parameter mono_core.backbone.layers_0.blocks.1.mlp.fc2.weight is trainable\n",
      "Parameter mono_core.backbone.layers_0.blocks.1.mlp.fc2.bias is trainable\n",
      "Parameter mono_core.backbone.layers_1.downsample.norm.weight is trainable\n",
      "Parameter mono_core.backbone.layers_1.downsample.norm.bias is trainable\n",
      "Parameter mono_core.backbone.layers_1.downsample.reduction.weight is trainable\n",
      "Parameter mono_core.backbone.layers_1.blocks.0.norm1.weight is trainable\n",
      "Parameter mono_core.backbone.layers_1.blocks.0.norm1.bias is trainable\n",
      "Parameter mono_core.backbone.layers_1.blocks.0.attn.relative_position_bias_table is trainable\n",
      "Parameter mono_core.backbone.layers_1.blocks.0.attn.qkv.weight is trainable\n",
      "Parameter mono_core.backbone.layers_1.blocks.0.attn.qkv.bias is trainable\n",
      "Parameter mono_core.backbone.layers_1.blocks.0.attn.proj.weight is trainable\n",
      "Parameter mono_core.backbone.layers_1.blocks.0.attn.proj.bias is trainable\n",
      "Parameter mono_core.backbone.layers_1.blocks.0.norm2.weight is trainable\n",
      "Parameter mono_core.backbone.layers_1.blocks.0.norm2.bias is trainable\n",
      "Parameter mono_core.backbone.layers_1.blocks.0.mlp.fc1.weight is trainable\n",
      "Parameter mono_core.backbone.layers_1.blocks.0.mlp.fc1.bias is trainable\n",
      "Parameter mono_core.backbone.layers_1.blocks.0.mlp.fc2.weight is trainable\n",
      "Parameter mono_core.backbone.layers_1.blocks.0.mlp.fc2.bias is trainable\n",
      "Parameter mono_core.backbone.layers_1.blocks.1.norm1.weight is trainable\n",
      "Parameter mono_core.backbone.layers_1.blocks.1.norm1.bias is trainable\n",
      "Parameter mono_core.backbone.layers_1.blocks.1.attn.relative_position_bias_table is trainable\n",
      "Parameter mono_core.backbone.layers_1.blocks.1.attn.qkv.weight is trainable\n",
      "Parameter mono_core.backbone.layers_1.blocks.1.attn.qkv.bias is trainable\n",
      "Parameter mono_core.backbone.layers_1.blocks.1.attn.proj.weight is trainable\n",
      "Parameter mono_core.backbone.layers_1.blocks.1.attn.proj.bias is trainable\n",
      "Parameter mono_core.backbone.layers_1.blocks.1.norm2.weight is trainable\n",
      "Parameter mono_core.backbone.layers_1.blocks.1.norm2.bias is trainable\n",
      "Parameter mono_core.backbone.layers_1.blocks.1.mlp.fc1.weight is trainable\n",
      "Parameter mono_core.backbone.layers_1.blocks.1.mlp.fc1.bias is trainable\n",
      "Parameter mono_core.backbone.layers_1.blocks.1.mlp.fc2.weight is trainable\n",
      "Parameter mono_core.backbone.layers_1.blocks.1.mlp.fc2.bias is trainable\n",
      "Parameter mono_core.backbone.layers_2.downsample.norm.weight is trainable\n",
      "Parameter mono_core.backbone.layers_2.downsample.norm.bias is trainable\n",
      "Parameter mono_core.backbone.layers_2.downsample.reduction.weight is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.0.norm1.weight is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.0.norm1.bias is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.0.attn.relative_position_bias_table is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.0.attn.qkv.weight is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.0.attn.qkv.bias is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.0.attn.proj.weight is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.0.attn.proj.bias is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.0.norm2.weight is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.0.norm2.bias is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.0.mlp.fc1.weight is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.0.mlp.fc1.bias is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.0.mlp.fc2.weight is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.0.mlp.fc2.bias is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.1.norm1.weight is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.1.norm1.bias is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.1.attn.relative_position_bias_table is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.1.attn.qkv.weight is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.1.attn.qkv.bias is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.1.attn.proj.weight is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.1.attn.proj.bias is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.1.norm2.weight is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.1.norm2.bias is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.1.mlp.fc1.weight is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.1.mlp.fc1.bias is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.1.mlp.fc2.weight is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.1.mlp.fc2.bias is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.2.norm1.weight is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.2.norm1.bias is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.2.attn.relative_position_bias_table is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.2.attn.qkv.weight is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.2.attn.qkv.bias is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.2.attn.proj.weight is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.2.attn.proj.bias is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.2.norm2.weight is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.2.norm2.bias is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.2.mlp.fc1.weight is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.2.mlp.fc1.bias is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.2.mlp.fc2.weight is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.2.mlp.fc2.bias is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.3.norm1.weight is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.3.norm1.bias is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.3.attn.relative_position_bias_table is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.3.attn.qkv.weight is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.3.attn.qkv.bias is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.3.attn.proj.weight is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.3.attn.proj.bias is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.3.norm2.weight is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.3.norm2.bias is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.3.mlp.fc1.weight is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.3.mlp.fc1.bias is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.3.mlp.fc2.weight is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.3.mlp.fc2.bias is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.4.norm1.weight is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.4.norm1.bias is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.4.attn.relative_position_bias_table is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.4.attn.qkv.weight is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.4.attn.qkv.bias is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.4.attn.proj.weight is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.4.attn.proj.bias is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.4.norm2.weight is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.4.norm2.bias is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.4.mlp.fc1.weight is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.4.mlp.fc1.bias is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.4.mlp.fc2.weight is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.4.mlp.fc2.bias is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.5.norm1.weight is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.5.norm1.bias is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.5.attn.relative_position_bias_table is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.5.attn.qkv.weight is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.5.attn.qkv.bias is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.5.attn.proj.weight is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.5.attn.proj.bias is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.5.norm2.weight is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.5.norm2.bias is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.5.mlp.fc1.weight is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.5.mlp.fc1.bias is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.5.mlp.fc2.weight is trainable\n",
      "Parameter mono_core.backbone.layers_2.blocks.5.mlp.fc2.bias is trainable\n",
      "Parameter mono_core.backbone.layers_3.downsample.norm.weight is trainable\n",
      "Parameter mono_core.backbone.layers_3.downsample.norm.bias is trainable\n",
      "Parameter mono_core.backbone.layers_3.downsample.reduction.weight is trainable\n",
      "Parameter mono_core.backbone.layers_3.blocks.0.norm1.weight is trainable\n",
      "Parameter mono_core.backbone.layers_3.blocks.0.norm1.bias is trainable\n",
      "Parameter mono_core.backbone.layers_3.blocks.0.attn.relative_position_bias_table is trainable\n",
      "Parameter mono_core.backbone.layers_3.blocks.0.attn.qkv.weight is trainable\n",
      "Parameter mono_core.backbone.layers_3.blocks.0.attn.qkv.bias is trainable\n",
      "Parameter mono_core.backbone.layers_3.blocks.0.attn.proj.weight is trainable\n",
      "Parameter mono_core.backbone.layers_3.blocks.0.attn.proj.bias is trainable\n",
      "Parameter mono_core.backbone.layers_3.blocks.0.norm2.weight is trainable\n",
      "Parameter mono_core.backbone.layers_3.blocks.0.norm2.bias is trainable\n",
      "Parameter mono_core.backbone.layers_3.blocks.0.mlp.fc1.weight is trainable\n",
      "Parameter mono_core.backbone.layers_3.blocks.0.mlp.fc1.bias is trainable\n",
      "Parameter mono_core.backbone.layers_3.blocks.0.mlp.fc2.weight is trainable\n",
      "Parameter mono_core.backbone.layers_3.blocks.0.mlp.fc2.bias is trainable\n",
      "Parameter mono_core.backbone.layers_3.blocks.1.norm1.weight is trainable\n",
      "Parameter mono_core.backbone.layers_3.blocks.1.norm1.bias is trainable\n",
      "Parameter mono_core.backbone.layers_3.blocks.1.attn.relative_position_bias_table is trainable\n",
      "Parameter mono_core.backbone.layers_3.blocks.1.attn.qkv.weight is trainable\n",
      "Parameter mono_core.backbone.layers_3.blocks.1.attn.qkv.bias is trainable\n",
      "Parameter mono_core.backbone.layers_3.blocks.1.attn.proj.weight is trainable\n",
      "Parameter mono_core.backbone.layers_3.blocks.1.attn.proj.bias is trainable\n",
      "Parameter mono_core.backbone.layers_3.blocks.1.norm2.weight is trainable\n",
      "Parameter mono_core.backbone.layers_3.blocks.1.norm2.bias is trainable\n",
      "Parameter mono_core.backbone.layers_3.blocks.1.mlp.fc1.weight is trainable\n",
      "Parameter mono_core.backbone.layers_3.blocks.1.mlp.fc1.bias is trainable\n",
      "Parameter mono_core.backbone.layers_3.blocks.1.mlp.fc2.weight is trainable\n",
      "Parameter mono_core.backbone.layers_3.blocks.1.mlp.fc2.bias is trainable\n",
      "Parameter mono_core.csa_1_8.proj_query.weight is trainable\n",
      "Parameter mono_core.csa_1_8.proj_query.bias is trainable\n",
      "Parameter mono_core.csa_1_8.proj_coarser.weight is trainable\n",
      "Parameter mono_core.csa_1_8.proj_coarser.bias is trainable\n",
      "Parameter mono_core.csa_1_16.proj_query.weight is trainable\n",
      "Parameter mono_core.csa_1_16.proj_query.bias is trainable\n",
      "Parameter mono_core.csa_1_16.proj_coarser.weight is trainable\n",
      "Parameter mono_core.csa_1_16.proj_coarser.bias is trainable\n",
      "Parameter mono_core.msr_levels.0.mlp1.0.weight is trainable\n",
      "Parameter mono_core.msr_levels.0.mlp1.0.bias is trainable\n",
      "Parameter mono_core.msr_levels.0.mlp2.0.weight is trainable\n",
      "Parameter mono_core.msr_levels.0.mlp2.0.bias is trainable\n",
      "Parameter mono_core.msr_levels.1.mlp1.0.weight is trainable\n",
      "Parameter mono_core.msr_levels.1.mlp1.0.bias is trainable\n",
      "Parameter mono_core.msr_levels.1.mlp2.0.weight is trainable\n",
      "Parameter mono_core.msr_levels.1.mlp2.0.bias is trainable\n",
      "number of trained parameters of the model: 28191610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 240 | Iteration: 399  | Running loss: 0.06643 | eta:32.93h\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 240 | Running loss: 0.06621 | eta:32.85h\n",
      "Save checkpoint at epoch 241 successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 241 | Running loss: 0.06753 | eta:32.47h\n",
      "Save checkpoint at epoch 242 successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 242 | Running loss: 0.06711 | eta:32.28h\n",
      "Save checkpoint at epoch 243 successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 243 | Running loss: 0.06684 | eta:32.11h\n",
      "Save checkpoint at epoch 244 successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 244 | Running loss: 0.06735 | eta:31.95h\n",
      "Save checkpoint at epoch 245 successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 245 | Running loss: 0.06687 | eta:31.80h\n",
      "Save checkpoint at epoch 246 successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 246 | Running loss: 0.06728 | eta:31.66h\n",
      "Save checkpoint at epoch 247 successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 247 | Running loss: 0.06644 | eta:31.52h\n",
      "Save checkpoint at epoch 248 successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 248 | Running loss: 0.06636 | eta:31.38h\n",
      "Save checkpoint at epoch 249 successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 249 | Running loss: 0.06715 | eta:31.23h\n",
      "Save checkpoint at epoch 250 successfully!\n",
      "\n",
      "/**** start testing after training epoch 249 ******/\n",
      "rebuild /kaggle/working/MonoDTR/output/validation/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3769 [00:00<?, ?it/s]/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:167: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds\n",
      "/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/networks/pipelines/testers.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  scores, bbox, obj_index = module([images.cuda().float().contiguous(), torch.tensor(P2).cuda().float()])\n",
      "100%|██████████| 3769/3769 [05:51<00:00, 10.71it/s]\n",
      "/usr/local/lib/python3.10/dist-packages/numba_cuda/numba/cuda/dispatcher.py:579: NumbaPerformanceWarning: \u001b[1mGrid size 36 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.10/dist-packages/numba_cuda/numba/cuda/dispatcher.py:579: NumbaPerformanceWarning: \u001b[1mGrid size 45 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.10/dist-packages/numba_cuda/numba/cuda/dispatcher.py:579: NumbaPerformanceWarning: \u001b[1mGrid size 60 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.10/dist-packages/numba_cuda/numba/cuda/dispatcher.py:579: NumbaPerformanceWarning: \u001b[1mGrid size 50 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.10/dist-packages/numba_cuda/numba/cuda/dispatcher.py:579: NumbaPerformanceWarning: \u001b[1mGrid size 32 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.10/dist-packages/numba_cuda/numba/cuda/dispatcher.py:579: NumbaPerformanceWarning: \u001b[1mGrid size 40 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.10/dist-packages/numba_cuda/numba/cuda/dispatcher.py:579: NumbaPerformanceWarning: \u001b[1mGrid size 6 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.10/dist-packages/numba/core/typed_passes.py:336: NumbaPerformanceWarning: \u001b[1m\n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see https://numba.readthedocs.io/en/stable/user/parallel.html#diagnostics for help.\n",
      "\u001b[1m\n",
      "File \"../input/monodtr-library-ver2/MonoDTR/visualDet3D/evaluator/kitti/eval.py\", line 129:\u001b[0m\n",
      "\u001b[1m@numba.jit(nopython=True, parallel=True)\n",
      "\u001b[1mdef d3_box_overlap_kernel(boxes,\n",
      "\u001b[0m\u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaPerformanceWarning(msg,\n",
      "/usr/local/lib/python3.10/dist-packages/numba_cuda/numba/cuda/dispatcher.py:579: NumbaPerformanceWarning: \u001b[1mGrid size 45 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.10/dist-packages/numba_cuda/numba/cuda/dispatcher.py:579: NumbaPerformanceWarning: \u001b[1mGrid size 60 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.10/dist-packages/numba_cuda/numba/cuda/dispatcher.py:579: NumbaPerformanceWarning: \u001b[1mGrid size 36 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.10/dist-packages/numba_cuda/numba/cuda/dispatcher.py:579: NumbaPerformanceWarning: \u001b[1mGrid size 50 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.10/dist-packages/numba_cuda/numba/cuda/dispatcher.py:579: NumbaPerformanceWarning: \u001b[1mGrid size 32 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.10/dist-packages/numba_cuda/numba/cuda/dispatcher.py:579: NumbaPerformanceWarning: \u001b[1mGrid size 40 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.10/dist-packages/numba_cuda/numba/cuda/dispatcher.py:579: NumbaPerformanceWarning: \u001b[1mGrid size 6 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Car AP(Average Precision)@0.70, 0.70, 0.70:\n",
      "bbox AP:95.57, 86.99, 77.06\n",
      "bev  AP:24.74, 18.70, 15.81\n",
      "3d   AP:16.77, 13.23, 11.11\n",
      "aos  AP:91.20, 81.41, 71.40\n",
      "Car AP(Average Precision)@0.70, 0.50, 0.50:\n",
      "bbox AP:95.57, 86.99, 77.06\n",
      "bev  AP:60.43, 44.96, 38.71\n",
      "3d   AP:54.00, 41.21, 35.27\n",
      "aos  AP:91.20, 81.41, 71.40\n",
      "\n",
      "/**** finish testing after training epoch 249 ******/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 250 | Running loss: 0.06694 | eta:33.40h\n",
      "Save checkpoint at epoch 251 successfully!\n",
      "Epoch: 251 | Running loss: 0.06773 | eta:33.05h\n",
      "Save checkpoint at epoch 252 successfully!\n",
      "Epoch: 252 | Running loss: 0.06703 | eta:32.74h\n",
      "Save checkpoint at epoch 253 successfully!\n",
      "Epoch: 253 | Running loss: 0.06570 | eta:32.45h\n",
      "Save checkpoint at epoch 254 successfully!\n",
      "Epoch: 254 | Running loss: 0.06641 | eta:32.19h\n",
      "Save checkpoint at epoch 255 successfully!\n",
      "Epoch: 255 | Running loss: 0.06702 | eta:31.94h\n",
      "Save checkpoint at epoch 256 successfully!\n",
      "Epoch: 256 | Running loss: 0.06556 | eta:31.71h\n",
      "Save checkpoint at epoch 257 successfully!\n",
      "Epoch: 257 | Running loss: 0.06613 | eta:31.48h\n",
      "Save checkpoint at epoch 258 successfully!\n",
      "Epoch: 258 | Running loss: 0.06605 | eta:31.27h\n",
      "Save checkpoint at epoch 259 successfully!\n",
      "Epoch: 259 | Running loss: 0.06644 | eta:31.06h\n",
      "Save checkpoint at epoch 260 successfully!\n",
      "\n",
      "/**** start testing after training epoch 259 ******/\n",
      "clean up the recorder directory of /kaggle/working/MonoDTR/output/validation/data\n",
      "rebuild /kaggle/working/MonoDTR/output/validation/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3769 [00:00<?, ?it/s]/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/networks/pipelines/testers.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  scores, bbox, obj_index = module([images.cuda().float().contiguous(), torch.tensor(P2).cuda().float()])\n",
      "100%|██████████| 3769/3769 [04:46<00:00, 13.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Car AP(Average Precision)@0.70, 0.70, 0.70:\n",
      "bbox AP:97.51, 86.97, 77.03\n",
      "bev  AP:24.62, 18.67, 15.73\n",
      "3d   AP:16.74, 12.82, 11.09\n",
      "aos  AP:93.01, 81.32, 71.28\n",
      "Car AP(Average Precision)@0.70, 0.50, 0.50:\n",
      "bbox AP:97.51, 86.97, 77.03\n",
      "bev  AP:60.34, 44.99, 38.76\n",
      "3d   AP:55.26, 41.27, 35.32\n",
      "aos  AP:93.01, 81.32, 71.28\n",
      "\n",
      "/**** finish testing after training epoch 259 ******/\n",
      "Epoch: 260 | Running loss: 0.06561 | eta:31.72h\n",
      "Save checkpoint at epoch 261 successfully!\n",
      "Epoch: 261 | Running loss: 0.06647 | eta:31.48h\n",
      "Save checkpoint at epoch 262 successfully!\n",
      "Epoch: 262 | Running loss: 0.06620 | eta:31.25h\n",
      "Save checkpoint at epoch 263 successfully!\n",
      "Epoch: 263 | Running loss: 0.06618 | eta:31.03h\n",
      "Save checkpoint at epoch 264 successfully!\n",
      "Epoch: 264 | Running loss: 0.06664 | eta:30.82h\n",
      "Save checkpoint at epoch 265 successfully!\n",
      "Epoch: 265 | Running loss: 0.06596 | eta:30.61h\n",
      "Save checkpoint at epoch 266 successfully!\n",
      "Epoch: 266 | Running loss: 0.06628 | eta:30.41h\n",
      "Save checkpoint at epoch 267 successfully!\n",
      "Epoch: 267 | Running loss: 0.06695 | eta:30.21h\n",
      "Save checkpoint at epoch 268 successfully!\n",
      "Epoch: 268 | Running loss: 0.06609 | eta:30.02h\n",
      "Save checkpoint at epoch 269 successfully!\n",
      "Epoch: 269 | Running loss: 0.06534 | eta:29.83h\n",
      "Save checkpoint at epoch 270 successfully!\n",
      "\n",
      "/**** start testing after training epoch 269 ******/\n",
      "clean up the recorder directory of /kaggle/working/MonoDTR/output/validation/data\n",
      "rebuild /kaggle/working/MonoDTR/output/validation/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3769/3769 [04:51<00:00, 12.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Car AP(Average Precision)@0.70, 0.70, 0.70:\n",
      "bbox AP:95.46, 86.94, 76.99\n",
      "bev  AP:25.25, 19.16, 16.13\n",
      "3d   AP:18.02, 13.81, 11.47\n",
      "aos  AP:90.90, 81.12, 71.05\n",
      "Car AP(Average Precision)@0.70, 0.50, 0.50:\n",
      "bbox AP:95.46, 86.94, 76.99\n",
      "bev  AP:60.72, 45.29, 39.02\n",
      "3d   AP:55.80, 41.63, 35.66\n",
      "aos  AP:90.90, 81.12, 71.05\n",
      "\n",
      "/**** finish testing after training epoch 269 ******/\n",
      "Epoch: 270 | Running loss: 0.06641 | eta:30.22h\n",
      "Save checkpoint at epoch 271 successfully!\n",
      "Epoch: 271 | Running loss: 0.06746 | eta:30.01h\n",
      "Save checkpoint at epoch 272 successfully!\n",
      "Epoch: 272 | Running loss: 0.06663 | eta:29.82h\n",
      "Save checkpoint at epoch 273 successfully!\n",
      "Epoch: 273 | Running loss: 0.06606 | eta:29.62h\n",
      "Save checkpoint at epoch 274 successfully!\n",
      "Epoch: 274 | Running loss: 0.06579 | eta:29.43h\n",
      "Save checkpoint at epoch 275 successfully!\n",
      "Epoch: 275 | Running loss: 0.06696 | eta:29.24h\n",
      "Save checkpoint at epoch 276 successfully!\n",
      "Epoch: 276 | Running loss: 0.06631 | eta:29.06h\n",
      "Save checkpoint at epoch 277 successfully!\n",
      "Epoch: 277 | Running loss: 0.06541 | eta:28.88h\n",
      "Save checkpoint at epoch 278 successfully!\n",
      "Epoch: 278 | Running loss: 0.06528 | eta:28.70h\n",
      "Save checkpoint at epoch 279 successfully!\n",
      "Epoch: 279 | Running loss: 0.06541 | eta:28.52h\n",
      "Save checkpoint at epoch 280 successfully!\n",
      "\n",
      "/**** start testing after training epoch 279 ******/\n",
      "clean up the recorder directory of /kaggle/working/MonoDTR/output/validation/data\n",
      "rebuild /kaggle/working/MonoDTR/output/validation/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3769/3769 [04:48<00:00, 13.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Car AP(Average Precision)@0.70, 0.70, 0.70:\n",
      "bbox AP:95.27, 86.97, 77.01\n",
      "bev  AP:24.96, 19.00, 15.96\n",
      "3d   AP:17.53, 13.56, 11.26\n",
      "aos  AP:90.83, 81.23, 71.16\n",
      "Car AP(Average Precision)@0.70, 0.50, 0.50:\n",
      "bbox AP:95.27, 86.97, 77.01\n",
      "bev  AP:60.75, 45.18, 38.90\n",
      "3d   AP:55.54, 41.45, 35.45\n",
      "aos  AP:90.83, 81.23, 71.16\n",
      "\n",
      "/**** finish testing after training epoch 279 ******/\n",
      "Epoch: 280 | Running loss: 0.06595 | eta:28.74h\n",
      "Save checkpoint at epoch 281 successfully!\n",
      "Epoch: 281 | Running loss: 0.06648 | eta:28.56h\n",
      "Save checkpoint at epoch 282 successfully!\n",
      "Epoch: 282 | Running loss: 0.06455 | eta:28.37h\n",
      "Save checkpoint at epoch 283 successfully!\n",
      "Epoch: 283 | Running loss: 0.06450 | eta:28.19h\n",
      "Save checkpoint at epoch 284 successfully!\n",
      "Epoch: 284 | Running loss: 0.06559 | eta:28.01h\n",
      "Save checkpoint at epoch 285 successfully!\n",
      "Epoch: 285 | Running loss: 0.06577 | eta:27.83h\n",
      "Save checkpoint at epoch 286 successfully!\n",
      "Epoch: 286 | Running loss: 0.06552 | eta:27.65h\n",
      "Save checkpoint at epoch 287 successfully!\n",
      "Epoch: 287 | Running loss: 0.06538 | eta:27.48h\n",
      "Save checkpoint at epoch 288 successfully!\n",
      "Epoch: 288 | Running loss: 0.06568 | eta:27.31h\n",
      "Save checkpoint at epoch 289 successfully!\n",
      "Epoch: 289 | Running loss: 0.06539 | eta:27.13h\n",
      "Save checkpoint at epoch 290 successfully!\n",
      "\n",
      "/**** start testing after training epoch 289 ******/\n",
      "clean up the recorder directory of /kaggle/working/MonoDTR/output/validation/data\n",
      "rebuild /kaggle/working/MonoDTR/output/validation/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3769/3769 [04:42<00:00, 13.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Car AP(Average Precision)@0.70, 0.70, 0.70:\n",
      "bbox AP:95.26, 86.96, 77.02\n",
      "bev  AP:24.67, 18.74, 15.80\n",
      "3d   AP:16.83, 13.36, 11.14\n",
      "aos  AP:90.85, 81.32, 71.23\n",
      "Car AP(Average Precision)@0.70, 0.50, 0.50:\n",
      "bbox AP:95.26, 86.96, 77.02\n",
      "bev  AP:60.76, 45.14, 38.86\n",
      "3d   AP:55.46, 41.37, 35.41\n",
      "aos  AP:90.85, 81.32, 71.23\n",
      "\n",
      "/**** finish testing after training epoch 289 ******/\n",
      "Epoch: 290 | Running loss: 0.06585 | eta:27.26h\n",
      "Save checkpoint at epoch 291 successfully!\n",
      "Epoch: 291 | Running loss: 0.06581 | eta:27.09h\n",
      "Save checkpoint at epoch 292 successfully!\n",
      "Epoch: 292 | Running loss: 0.06472 | eta:26.91h\n",
      "Save checkpoint at epoch 293 successfully!\n",
      "Epoch: 293 | Running loss: 0.06523 | eta:26.74h\n",
      "Save checkpoint at epoch 294 successfully!\n",
      "Epoch: 294 | Running loss: 0.06627 | eta:26.57h\n",
      "Save checkpoint at epoch 295 successfully!\n",
      "Epoch: 295 | Running loss: 0.06728 | eta:26.40h\n",
      "Save checkpoint at epoch 296 successfully!\n",
      "Epoch: 296 | Running loss: 0.06630 | eta:26.23h\n",
      "Save checkpoint at epoch 297 successfully!\n",
      "Epoch: 297 | Running loss: 0.06577 | eta:26.06h\n",
      "Save checkpoint at epoch 298 successfully!\n",
      "Epoch: 298 | Running loss: 0.06586 | eta:25.89h\n",
      "Save checkpoint at epoch 299 successfully!\n",
      "Epoch: 299 | Running loss: 0.06740 | eta:25.73h\n",
      "Save checkpoint at epoch 300 successfully!\n",
      "\n",
      "/**** start testing after training epoch 299 ******/\n",
      "clean up the recorder directory of /kaggle/working/MonoDTR/output/validation/data\n",
      "rebuild /kaggle/working/MonoDTR/output/validation/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3769/3769 [04:46<00:00, 13.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Car AP(Average Precision)@0.70, 0.70, 0.70:\n",
      "bbox AP:95.53, 86.96, 77.05\n",
      "bev  AP:23.57, 18.12, 15.36\n",
      "3d   AP:16.71, 12.64, 10.28\n",
      "aos  AP:90.90, 81.16, 71.20\n",
      "Car AP(Average Precision)@0.70, 0.50, 0.50:\n",
      "bbox AP:95.53, 86.96, 77.05\n",
      "bev  AP:59.77, 44.53, 38.37\n",
      "3d   AP:53.41, 40.53, 34.78\n",
      "aos  AP:90.90, 81.16, 71.20\n",
      "\n",
      "/**** finish testing after training epoch 299 ******/\n"
     ]
    }
   ],
   "source": [
    "run_training()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1357458,
     "sourceId": 2255937,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7116330,
     "isSourceIdPinned": true,
     "sourceId": 11368341,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7365741,
     "sourceId": 11733251,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7110160,
     "isSourceIdPinned": true,
     "sourceId": 12735433,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7381980,
     "sourceId": 12808421,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8226325,
     "sourceId": 13370291,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 31224.055756,
   "end_time": "2025-10-13T22:58:53.831390",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-13T14:18:29.775634",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "16c7f4a66e884eeba2b64d778cd6d8e5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "25178a70168d474cbb1453785d6c14f9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_8ba5950ee335403fb9778d28b9664257",
        "IPY_MODEL_ff528a0c015f4246a6d83b265922da1d",
        "IPY_MODEL_cc3d4d159f724a14b132f564f6253139"
       ],
       "layout": "IPY_MODEL_8ec2dca330264358b3588e505e6fe33b",
       "tabbable": null,
       "tooltip": null
      }
     },
     "40006f6d29744e40b1e89e110f58db6f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8ba5950ee335403fb9778d28b9664257": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_92beb35460534c4897be3bb8697b3d62",
       "placeholder": "​",
       "style": "IPY_MODEL_97a2791d1018441d99e68708f9286007",
       "tabbable": null,
       "tooltip": null,
       "value": "model.safetensors: 100%"
      }
     },
     "8ec2dca330264358b3588e505e6fe33b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "92beb35460534c4897be3bb8697b3d62": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "97a2791d1018441d99e68708f9286007": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b543acc5f7dd498eb60f04abf2fce3cc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "cc3d4d159f724a14b132f564f6253139": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_40006f6d29744e40b1e89e110f58db6f",
       "placeholder": "​",
       "style": "IPY_MODEL_ec38a9d9b89c4daeb07deff50f74d282",
       "tabbable": null,
       "tooltip": null,
       "value": " 114M/114M [00:00&lt;00:00, 258MB/s]"
      }
     },
     "ec38a9d9b89c4daeb07deff50f74d282": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ff528a0c015f4246a6d83b265922da1d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_16c7f4a66e884eeba2b64d778cd6d8e5",
       "max": 114286722.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b543acc5f7dd498eb60f04abf2fce3cc",
       "tabbable": null,
       "tooltip": null,
       "value": 114286722.0
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
