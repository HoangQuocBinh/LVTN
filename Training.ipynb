{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2255937,"sourceType":"datasetVersion","datasetId":1357458},{"sourceId":11368341,"sourceType":"datasetVersion","datasetId":7116330,"isSourceIdPinned":true},{"sourceId":11733251,"sourceType":"datasetVersion","datasetId":7365741},{"sourceId":12735433,"sourceType":"datasetVersion","datasetId":7110160,"isSourceIdPinned":true},{"sourceId":12808421,"sourceType":"datasetVersion","datasetId":7381980}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!python --version","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T14:46:08.644388Z","iopub.execute_input":"2025-08-19T14:46:08.644700Z","iopub.status.idle":"2025-08-19T14:46:08.770208Z","shell.execute_reply.started":"2025-08-19T14:46:08.644657Z","shell.execute_reply":"2025-08-19T14:46:08.769154Z"}},"outputs":[{"name":"stdout","text":"Python 3.10.12\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import sys\n\nsys.path.append(\"/kaggle/input/monodtr-library-ver2/MonoDTR\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T14:46:08.771378Z","iopub.execute_input":"2025-08-19T14:46:08.771727Z","iopub.status.idle":"2025-08-19T14:46:08.775980Z","shell.execute_reply.started":"2025-08-19T14:46:08.771678Z","shell.execute_reply":"2025-08-19T14:46:08.774861Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!pip install -qq -r /kaggle/input/monodtr-library-ver2/MonoDTR/requirement.txt\n!pip install -qq coloredlogs\n!pip install -qq ptflops","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T14:46:08.776858Z","iopub.execute_input":"2025-08-19T14:46:08.777131Z","iopub.status.idle":"2025-08-19T14:46:18.986883Z","shell.execute_reply.started":"2025-08-19T14:46:08.777109Z","shell.execute_reply":"2025-08-19T14:46:18.985719Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"sys.path.append(\"/kaggle/input/monodtr-library-ver2/MonoDTR/scripts\")\nsys.path.append(\"/kaggle/input/pretrain-monodtr-base\")\nsys.path.append(\"/kaggle/input/checkpoint\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T14:46:18.987878Z","iopub.execute_input":"2025-08-19T14:46:18.988121Z","iopub.status.idle":"2025-08-19T14:46:18.992380Z","shell.execute_reply.started":"2025-08-19T14:46:18.988100Z","shell.execute_reply":"2025-08-19T14:46:18.991347Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import os\nimport sys\nimport torch\n\n# Add the directory containing the .so file to the system path\nsys.path.append('/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/networks/lib/ops/dcn')\n\n# Check if the .so file exists\nprint(os.listdir('/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/networks/lib/ops/dcn'))\n\n# Print Python version\nprint(\"Python version:\", sys.version)\n\n# Set LD_LIBRARY_PATH\nos.environ['LD_LIBRARY_PATH'] = '/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/networks/lib/ops/dcn:' + os.environ.get('LD_LIBRARY_PATH', '')\n\n# Attempt to import the module\ntry:\n    import deform_conv_ext\n    print(\"Import successful!\")\nexcept ImportError as e:\n    print(\"Import failed:\", e)\n\n# If needed, run the setup or make script\n# !python setup.py build\n# !python setup.py install\n# or\n# !bash make.sh\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T14:46:18.994480Z","iopub.execute_input":"2025-08-19T14:46:18.994722Z","iopub.status.idle":"2025-08-19T14:46:20.661595Z","shell.execute_reply.started":"2025-08-19T14:46:18.994703Z","shell.execute_reply":"2025-08-19T14:46:20.660678Z"}},"outputs":[{"name":"stdout","text":"['deform_conv.py', 'deform_conv_ext.cpython-310-x86_64-linux-gnu.so', 'make.sh', 'src', '__init__.py', 'setup.py']\nPython version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]\nImport successful!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch\n\nif torch.cuda.is_available():\n    print(\"CUDA is available!\")\n    print(\"Number of GPUs available:\", torch.cuda.device_count())\n    print(\"Current GPU:\", torch.cuda.current_device())\n    print(\"GPU Name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\nelse:\n    print(\"CUDA is not available. Check your settings.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T14:46:20.663001Z","iopub.execute_input":"2025-08-19T14:46:20.663340Z","iopub.status.idle":"2025-08-19T14:46:20.709423Z","shell.execute_reply.started":"2025-08-19T14:46:20.663318Z","shell.execute_reply":"2025-08-19T14:46:20.708744Z"}},"outputs":[{"name":"stdout","text":"CUDA is available!\nNumber of GPUs available: 1\nCurrent GPU: 0\nGPU Name: Tesla P100-PCIE-16GB\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Create a large tensor and move it to the GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nx = torch.randn(10000, 10000, device=device)  # This should allocate GPU memory\nprint(\"Tensor created on GPU.\")\nprint(\"Allocated GPU Memory:\", torch.cuda.memory_allocated(device))\nprint(\"Cached GPU Memory:\", torch.cuda.memory_reserved(device))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T14:46:20.710174Z","iopub.execute_input":"2025-08-19T14:46:20.710454Z","iopub.status.idle":"2025-08-19T14:46:20.823759Z","shell.execute_reply.started":"2025-08-19T14:46:20.710431Z","shell.execute_reply":"2025-08-19T14:46:20.822912Z"}},"outputs":[{"name":"stdout","text":"Tensor created on GPU.\nAllocated GPU Memory: 400556032\nCached GPU Memory: 400556032\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import torch.nn as nn\nimport torch\nimport torch\nfrom torch.nn import Module, Dropout\nfrom einops import rearrange\nimport torch.nn.functional as F\nfrom ptflops import get_model_complexity_info","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T14:46:20.824639Z","iopub.execute_input":"2025-08-19T14:46:20.824933Z","iopub.status.idle":"2025-08-19T14:46:22.913783Z","shell.execute_reply.started":"2025-08-19T14:46:20.824900Z","shell.execute_reply":"2025-08-19T14:46:22.912934Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# -------------------------- RMSNorm --------------------------\nclass RMSNorm(nn.Module):\n    def __init__(self, dim, eps=1e-8):\n        super().__init__()\n        self.eps = eps\n        self.scale = nn.Parameter(torch.ones(dim))\n\n    def forward(self, x):\n        # x: (B, L, D)\n        norm_x = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n        return self.scale * norm_x\n\n# -------------------------- FLIP Module --------------------------\ndef flip(x, dim):\n    return torch.flip(x, dims=[dim])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T14:46:22.914640Z","iopub.execute_input":"2025-08-19T14:46:22.914925Z","iopub.status.idle":"2025-08-19T14:46:22.921035Z","shell.execute_reply.started":"2025-08-19T14:46:22.914901Z","shell.execute_reply":"2025-08-19T14:46:22.919840Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# -------------------------- BiMamba2 Block (Conv Fusion) --------------------------\nclass BiMamba2(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.linear_x = nn.Linear(dim, dim)\n        self.linear_bf = nn.Linear(dim, dim)\n        self.linear_bb = nn.Linear(dim, dim)\n        self.linear_z = nn.Linear(dim, dim)\n\n        self.concat_proj_f = nn.Linear(2 * dim, dim)\n        self.concat_proj_b = nn.Linear(2 * dim, dim)\n\n        self.conv1d_f = nn.Conv1d(dim, dim, kernel_size=1)\n        self.conv1d_b = nn.Conv1d(dim, dim, kernel_size=1)\n        self.conv1d_zf = nn.Conv1d(dim, dim, kernel_size=1)\n        self.conv1d_ab_z = nn.Conv1d(dim, dim, kernel_size=1)\n\n        self.ssms_f = nn.Sequential(\n            nn.Conv1d(dim, dim, 1),\n            nn.GELU(),\n            nn.Conv1d(dim, dim, 1)\n        )\n        self.ssms_b = nn.Sequential(\n            nn.Conv1d(dim, dim, 1),\n            nn.GELU(),\n            nn.Conv1d(dim, dim, 1)\n        )\n\n        self.norm_f = RMSNorm(dim)\n        self.norm_b = RMSNorm(dim)\n        self.output_linear = nn.Linear(dim, dim)\n\n    def forward(self, u):\n        # u: (B, L, D)\n        x = self.linear_x(u)          # (B, L, D)\n        bf = self.linear_bf(u)        # (B, L, D)\n        bb = self.linear_bb(u)        # (B, L, D)\n        z = self.linear_z(u)          # (B, L, D)\n\n        bf_x = torch.cat([bf, x], dim=-1)         # (B, L, 2D)\n        bb_x = torch.cat([bb, x], dim=-1)         # (B, L, 2D)\n\n        bf_x = self.concat_proj_f(bf_x)           # (B, L, D)\n        bb_x = self.concat_proj_b(bb_x)           # (B, L, D)\n\n        af = self.conv1d_f(bf_x.transpose(1, 2))  # (B, D, L)\n        af = F.gelu(af)     \n        af = self.ssms_f(af).transpose(1, 2)      # (B, L, D)# (B, D, L)\n        # af = af + self.conv1d_zf(z.transpose(1, 2))  # af conv with z\n        af = af * z\n        af = self.norm_f(af)                      # (B, L, D)\n\n        bb_x_flip = flip(bb_x, dim=1)             # (B, L, D)\n        ab = self.conv1d_b(bb_x_flip.transpose(1, 2))  # (B, D, L)\n        ab = F.gelu(ab)                                # (B, D, L)\n\n        z_flip = flip(z, dim=1)                        # (B, L, D)\n        ab = self.ssms_b(ab).transpose(1, 2)           # (B, L, D)\n        # ab = ab + self.conv1d_ab_z(z_flip.transpose(1, 2))  # (B, D, L)\n        ab = ab * z_flip   # (B, D, L)\n        ab = self.norm_b(ab)                           # (B, L, D)\n        ab = flip(ab, dim=1)                           # (B, L, D)\n\n        out = self.output_linear(af + ab)         # (B, L, D)                 # (B, L, D)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T14:46:22.921834Z","iopub.execute_input":"2025-08-19T14:46:22.922064Z","iopub.status.idle":"2025-08-19T14:46:22.943740Z","shell.execute_reply.started":"2025-08-19T14:46:22.922044Z","shell.execute_reply":"2025-08-19T14:46:22.942794Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# -------------------------- CrossMamba2 Module --------------------------\nclass CrossMamba2(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.linear_x = nn.Linear(dim, dim)\n        self.linear_z = nn.Linear(dim, dim)\n        self.linear_bf = nn.Linear(dim, dim)\n        self.linear_bb = nn.Linear(dim, dim)\n\n        self.concat_proj_f = nn.Linear(2 * dim, dim)\n        self.concat_proj_b = nn.Linear(2 * dim, dim)\n\n        self.conv1d_f = nn.Conv1d(dim, dim, kernel_size=1)\n        self.conv1d_b = nn.Conv1d(dim, dim, kernel_size=1)\n        self.conv1d_zf = nn.Conv1d(dim, dim, kernel_size=1)\n        self.conv1d_ab_z = nn.Conv1d(dim, dim, kernel_size=1)\n\n        self.ssms_f = nn.Sequential(\n            nn.Conv1d(dim, dim, 1),\n            nn.GELU(),\n            nn.Conv1d(dim, dim, 1)\n        )\n        self.ssms_b = nn.Sequential(\n            nn.Conv1d(dim, dim, 1),\n            nn.GELU(),\n            nn.Conv1d(dim, dim, 1)\n        )\n\n        self.norm_f = RMSNorm(dim)\n        self.norm_b = RMSNorm(dim)\n        self.output_linear = nn.Linear(dim, dim)\n\n    def forward(self, u1, u2):\n        # u1: context (B, L, D), u2: depth (B, L, D)\n        x = self.linear_x(u1)               # (B, L, D)\n        z = self.linear_z(u1)               # (B, L, D)\n        bf = self.linear_bf(u2)             # (B, L, D)\n        bb = self.linear_bb(u2)             # (B, L, D)\n\n        bf_x = torch.cat([bf, x], dim=-1)   # (B, L, 2D)\n        bb_x = torch.cat([bb, x], dim=-1)   # (B, L, 2D)\n\n        bf_x = self.concat_proj_f(bf_x)     # (B, L, D)\n        bb_x = self.concat_proj_b(bb_x)     # (B, L, D)\n\n        af = self.conv1d_f(bf_x.transpose(1, 2))  # (B, D, L)\n        af = F.gelu(af)\n        af = self.ssms_f(af).transpose(1, 2)      # (B, L, D)\n        # af = af + self.conv1d_zf(z.transpose(1, 2))  # af conv with z\n        af = af * z  # af conv with z\n        af = self.norm_f(af)\n\n        bb_x_flip = flip(bb_x, dim=1)             # (B, L, D)\n        ab = self.conv1d_b(bb_x_flip.transpose(1, 2))\n        ab = F.gelu(ab).transpose(1, 2)           # (B, L, D)\n\n        z_flip = flip(z, dim=1)\n        ab = self.ssms_b(ab.transpose(1, 2)).transpose(1, 2)\n        # ab = ab + self.conv1d_ab_z(z_flip.transpose(1, 2))  # (B, D, L)\n        ab = ab * z_flip  # (B, D, L)\n        ab = self.norm_b(ab)\n        ab = flip(ab, dim=1)\n\n        out = self.output_linear(af + ab)         # (B, L, D)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T14:46:22.944795Z","iopub.execute_input":"2025-08-19T14:46:22.945109Z","iopub.status.idle":"2025-08-19T14:46:22.967307Z","shell.execute_reply.started":"2025-08-19T14:46:22.945069Z","shell.execute_reply":"2025-08-19T14:46:22.966467Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# -------------------------- FFN + Norm --------------------------\nclass FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),\n            nn.Linear(hidden_dim, dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T14:46:22.968205Z","iopub.execute_input":"2025-08-19T14:46:22.968548Z","iopub.status.idle":"2025-08-19T14:46:22.988173Z","shell.execute_reply.started":"2025-08-19T14:46:22.968523Z","shell.execute_reply":"2025-08-19T14:46:22.987380Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# -------------------------- Depth Aware Mamba (DTR-Compatible Input) --------------------------\nclass DepthAwareMamba(nn.Module):\n    def __init__(self, output_channel_num):\n        super().__init__()\n        self.output_channel_num = output_channel_num\n\n        self.encoder_bimamba = BiMamba2(self.output_channel_num)\n        self.encoder_ffn = FeedForward(self.output_channel_num, self.output_channel_num * 2)\n        self.encoder_norm1 = nn.LayerNorm(self.output_channel_num)\n        self.encoder_norm2 = nn.LayerNorm(self.output_channel_num)\n\n        self.decoder_bimamba = BiMamba2(self.output_channel_num)\n        self.cross_bimamba = CrossMamba2(self.output_channel_num)\n        self.decoder_ffn = FeedForward(self.output_channel_num, self.output_channel_num * 2)\n        self.decoder_norm1 = nn.LayerNorm(self.output_channel_num)\n        self.decoder_norm2 = nn.LayerNorm(self.output_channel_num)\n        self.decoder_norm3 = nn.LayerNorm(self.output_channel_num)\n\n    def forward(self, depth_feat, context_feat, depth_pos=None):\n        depth_feat = depth_feat.contiguous()\n        context_feat = context_feat.contiguous()\n        if depth_pos is not None:\n            depth_pos = depth_pos.contiguous()\n            context_feat = context_feat + depth_pos\n    \n        # Encoder on context_feat\n        x = self.encoder_bimamba(context_feat.contiguous())\n        x = self.encoder_norm1((x + context_feat).contiguous())\n        x_ffn = self.encoder_ffn(x.contiguous())\n        x = self.encoder_norm2((x + x_ffn).contiguous())\n    \n        # Decoder on depth_feat and fused context\n        d = self.decoder_bimamba(depth_feat.contiguous())\n        d = self.decoder_norm1((d + depth_feat).contiguous())\n    \n        x = self.cross_bimamba(x.contiguous(), d.contiguous())\n        x = self.decoder_norm2(x.contiguous())\n        x_ffn = self.decoder_ffn(x.contiguous())\n        x = self.decoder_norm3((x + x_ffn).contiguous())\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T14:46:22.988976Z","iopub.execute_input":"2025-08-19T14:46:22.989300Z","iopub.status.idle":"2025-08-19T14:46:23.005026Z","shell.execute_reply.started":"2025-08-19T14:46:22.989275Z","shell.execute_reply":"2025-08-19T14:46:23.004244Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass CSA(nn.Module):\n    def __init__(self, in_channels):\n        super(CSA, self).__init__()\n        self.in_channels = in_channels\n\n    def forward(self, fine_feat, coarse_feat):\n        \"\"\"\n        Cross-Scale Attention (CSA) module.\n        \n        Inputs:\n            fine_feat: (N, C, H1, W1) - Finer resolution feature\n            coarse_feat: (N, C, H2, W2) - Coarser resolution feature (H2 < H1, W2 < W1)\n        \n        Output:\n            refined_feat: (N, C, H1, W1) - Refined feature at finer resolution\n        \"\"\"\n        N, C, H1, W1 = fine_feat.shape  # Finer resolution\n        _, _, H2, W2 = coarse_feat.shape  # Coarser resolution\n\n        # Upsample coarse features to match fine resolution\n        coarse_reshaped = F.interpolate(coarse_feat, size=(H1, W1), mode='bilinear', align_corners=False)\n        # coarse_reshaped: (N, C, H1, W1)\n\n        # Compute attention map\n        fine_flat = fine_feat.view(N, C, H1 * W1).permute(0, 2, 1)  # (N, H1*W1, C)\n        coarse_flat = coarse_reshaped.view(N, C, H1 * W1).permute(0, 2, 1)  # (N, H1*W1, C)\n        attention = torch.bmm(fine_flat, coarse_flat.transpose(1, 2))  # (N, H1*W1, H1*W1)\n        attention = F.softmax(attention, dim=-1)\n\n        # Apply attention to refine fine features\n        fine_attended = torch.bmm(attention, fine_flat).permute(0, 2, 1).view(N, C, H1, W1)\n        # fine_attended: (N, C, H1, W1)\n        refined_feat = fine_feat + fine_attended  # Residual connection\n        return refined_feat  # (N, C, H1, W1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T14:46:23.005786Z","iopub.execute_input":"2025-08-19T14:46:23.006011Z","iopub.status.idle":"2025-08-19T14:46:23.024128Z","shell.execute_reply.started":"2025-08-19T14:46:23.005991Z","shell.execute_reply":"2025-08-19T14:46:23.023299Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class MSR(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(MSR, self).__init__()\n        self.mlp = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=1),\n            nn.ReLU(),\n            nn.Conv2d(out_channels, out_channels, kernel_size=1)\n        )\n\n    def forward(self, input_feat, target_size, combine_feat=None):\n        \"\"\"\n        Multi-Scale Refinement (MSR) module.\n        \n        Inputs:\n            input_feat: (N, C_in, H_in, W_in) - Feature to upsample and refine\n            target_size: tuple (H_target, W_target) - Target spatial dimensions\n            combine_feat: (N, C_out, H_target, W_target) or None - Feature to combine with\n        \n        Output:\n            refined_feat: (N, C_out, H_target, W_target)\n        \"\"\"\n        # Upsample input_feat to target_size\n        upsampled = F.interpolate(input_feat, size=target_size, mode='bilinear', align_corners=False)\n        # upsampled: (N, C_in, H_target, W_target)\n        \n        # Apply MLP to refine\n        refined = self.mlp(upsampled)\n        # refined: (N, C_out, H_target, W_target)\n        \n        if combine_feat is not None:\n            if combine_feat.shape[1] != refined.shape[1]:\n                raise ValueError(\"Channel mismatch for combination\")\n            refined = refined + combine_feat  # Additive combination\n            # refined: (N, C_out, H_target, W_target)\n        return refined","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T14:46:23.025151Z","iopub.execute_input":"2025-08-19T14:46:23.025509Z","iopub.status.idle":"2025-08-19T14:46:23.044687Z","shell.execute_reply.started":"2025-08-19T14:46:23.025473Z","shell.execute_reply":"2025-08-19T14:46:23.043899Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"from visualDet3D.networks.backbones.dla import dla102\nfrom visualDet3D.networks.backbones.dlaup import DLAUp\nfrom visualDet3D.networks.detectors.dfe import DepthAwareFE\nfrom visualDet3D.networks.detectors.dpe import DepthAwarePosEnc\nfrom visualDet3D.networks.detectors.dtr import DepthAwareTransformer\nimport math\nimport time\nimport csv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T14:46:23.045556Z","iopub.execute_input":"2025-08-19T14:46:23.045881Z","iopub.status.idle":"2025-08-19T14:46:29.135622Z","shell.execute_reply.started":"2025-08-19T14:46:23.045848Z","shell.execute_reply":"2025-08-19T14:46:29.134864Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"import timm\n\nclass MonoDTRCore(nn.Module):\n    def __init__(self, backbone_arguments=dict()):\n        super(MonoDTRCore, self).__init__()\n\n        # Swin-T backbone with features_only=True to ensure feature pyramid output\n        self.backbone = timm.create_model('swin_tiny_patch4_window7_224', pretrained=True,features_only=True, img_size=(288, 1280))\n        # self.backbone = timm.create_model('swin_tiny_patch4_window7_224', pretrained=True, features_only=True)\n        self.first_level = 1  # Kept for reference, but no longer used\n\n        # Feature pyramid channels from Swin-T\n        self.channels = [96, 192, 384, 768]\n\n        # Projection layers to adjust channels to 256 for CSA and MSR\n        self.proj_1_4 = nn.Conv2d(96, 256, kernel_size=1)\n        self.dropout_1_4 = nn.Dropout(0.3)\n        self.proj_1_8 = nn.Conv2d(192, 256, kernel_size=1)\n        self.dropout_1_8 = nn.Dropout(0.3)\n        self.proj_1_16 = nn.Conv2d(384, 256, kernel_size=1)\n        self.dropout_1_16 = nn.Dropout(0.3)\n        self.proj_1_32 = nn.Conv2d(768, 256, kernel_size=1)\n        self.dropout_1_32 = nn.Dropout(0.3)\n\n        # CSA modules for cross-scale attention\n        self.csa_1_16_to_1_32 = CSA(256)  # 1/16 with 1/32\n        self.csa_1_8_to_1_16 = CSA(256)   # 1/8 with 1/16\n        self.csa_1_4_to_1_8 = CSA(256)    # 1/4 with 1/8\n\n        # MSR modules for upsampling and refinement\n        self.msr_1_16_to_1_8 = MSR(256, 256)  # Upsample 1/16 to 1/8, combine with CSA_2 (1/8)\n        self.msr_1_8_to_1_4 = MSR(256, 256)   # Upsample 1/8 to 1/4, combine with CSA_3 (1/4)\n        self.msr_1_4_to_1_2 = MSR(256, 256)   # Upsample 1/4 to 1/2, no combine_feat (0 input)\n        self.msr_1_2_to_1_1 = MSR(256, 256)   # Upsample 1/2 to 1/1, no combine_feat (0 input)\n\n        # Downsample for transformer input to manage memory usage\n        self.downsample_for_transformer = nn.Conv2d(256, 256, kernel_size=3, stride=8, padding=1)\n        self.dropout_transformer = nn.Dropout(0.3)\n        # Downstream modules (assumed from original MonoDTRCore)\n        self.output_channel_num = 256\n        self.dpe = DepthAwarePosEnc(self.output_channel_num)\n        self.depth_embed = nn.Embedding(100, self.output_channel_num)\n        self.dtr = DepthAwareTransformer(self.output_channel_num)\n        self.dfe = DepthAwareFE(self.output_channel_num)\n        self.img_conv = nn.Conv2d(self.output_channel_num, self.output_channel_num, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of MonoDTRCore.\n        \n        Input:\n            x: dict with key 'image' containing (N, 3, H, W)\n        \n        Output:\n            feat: (N, C, H/8, W/8) - Processed feature map for compatibility with dtr\n            depth: (N, D, H, W) - Depth prediction at 1/1 resolution\n        \"\"\"\n        if 'image' not in x:\n            raise ValueError(\"Input dictionary must contain 'image' key\")\n        img = x['image']  # (N, 3, H, W)\n        assert img.shape[2] == 288 and img.shape[3] == 1280, f\"Expected shape [N, 3, 288, 1280], got {img.shape}\"\n\n        if img.dim() != 4:\n            raise ValueError(f\"Expected 4D input tensor (N, C, H, W), got shape {img.shape}\")\n        \n        if img.shape[1] != 3:\n            raise ValueError(\n                f\"Expected input image to have 3 channels (RGB), got {img.shape[1]} channels. \"\n                \"Please check your data pipeline to ensure the input is correctly formatted as (N, 3, H, W). \"\n                \"If the input has 7 channels (e.g., multi-spectral), preprocess it to 3 channels or modify the backbone.\"\n            )\n\n        # Extract features using standard forward method\n        features = self.backbone(img)  # [(N, H/4, W/4, 96), (N, H/8, W/8, 192), (N, H/16, W/16, 384), (N, H/32, W/32, 768)]\n        # print(f\"Feature shapes: {[f.shape for f in features]}\")  # Debug\n        # Permute features from (N, H, W, C) to (N, C, H, W)\n        features = [f.permute(0, 3, 1, 2) for f in features]  # [(N, 96, H/4, W/4), (N, 192, H/8, W/8), (N, 384, H/16, W/16), (N, 768, H/32, W/32)]\n\n        # Apply projection layers with dropout\n        proj_1_4 = self.dropout_1_4(self.proj_1_4(features[0]))   # (N, 256, 72, 320)\n        proj_1_8 = self.dropout_1_8(self.proj_1_8(features[1]))   # (N, 256, 36, 160)\n        proj_1_16 = self.dropout_1_16(self.proj_1_16(features[2])) # (N, 256, 18, 80)\n        proj_1_32 = self.dropout_1_32(self.proj_1_32(features[3])) # (N, 256, 9, 40)\n\n        # Refine features progressively with CSA and MSR\n        feat_1_16 = self.csa_1_16_to_1_32(proj_1_16, proj_1_32)  # (N, 256, H/16, W/16)\n        feat_1_8 = self.csa_1_8_to_1_16(proj_1_8, feat_1_16)     # (N, 256, H/8, W/8)\n        feat_1_4 = self.csa_1_4_to_1_8(proj_1_4, feat_1_8)       # (N, 256, H/4, W/4)\n        feat_1_8 = feat_1_8 + self.msr_1_16_to_1_8(feat_1_16, (proj_1_8.shape[2], proj_1_8.shape[3]), combine_feat=feat_1_8)  # (N, 256, H/8, W/8)\n        feat_1_4 = feat_1_4 + self.msr_1_8_to_1_4(feat_1_8, (proj_1_4.shape[2], proj_1_4.shape[3]), combine_feat=feat_1_4)      # (N, 256, H/4, W/4)\n        target_size_1_2 = (feat_1_4.shape[2] * 2, feat_1_4.shape[3] * 2)\n        feat_1_2 = self.msr_1_4_to_1_2(feat_1_4, target_size_1_2)  # (N, 256, H/2, W/2)\n        target_size_1_1 = (feat_1_2.shape[2] * 2, feat_1_2.shape[3] * 2)\n        feat_1_1 = self.msr_1_2_to_1_1(feat_1_2, target_size_1_1)  # (N, 256, H, W)\n\n        # Keep feat_1_1 for depth prediction at 1/1 resolution\n        x_full_res = feat_1_1  # (N, 256, H, W)\n\n        # Downsample for transformer to manage memory usage and match original resolution\n        x = self.downsample_for_transformer(x_full_res) # (N, 256, 36, 160)\n\n        # Proceed with downstream processing\n        N, C, H, W = x.shape  # H=H, W=W (full resolution)\n        depth, depth_guide, depth_feat = self.dfe(x) # depth: (N, D, 288, 1280), depth_guide: (N, num_classes, 288, 1280), depth_feat: (N, 256, 288, 1280)\n        depth_feat = depth_feat.permute(0, 2, 3, 1).view(N, H * W, C)\n        depth_guide = depth_guide.argmax(1)\n        depth_emb = self.depth_embed(depth_guide).view(N, H * W, C)\n        depth_emb = self.dpe(depth_emb, (H, W))\n        img_feat = x + self.img_conv(x)\n        img_feat = img_feat.permute(0, 2, 3, 1)\n        # print(f\"img_feat shape: {img_feat.shape}\")\n        img_feat = img_feat.view(N, H * W, C)\n        feat = self.dtr(depth_feat, img_feat, depth_emb)\n        feat = feat.permute(0, 2, 1).view(N, C, H, W)\n\n        # N, C, H, W = x.shape\n        # depth, depth_guide, depth_feat = self.dfe(x_full_res)\n        # depth_feat = depth_feat.permute(0, 2, 3, 1).reshape(N, (x_full_res.shape[2] * x_full_res.shape[3]), C)\n        # depth_guide = depth_guide.argmax(1)\n        # depth_emb = self.depth_embed(depth_guide).reshape(N, (x_full_res.shape[2] * x_full_res.shape[3]), C)\n        # depth_emb = depth_emb.reshape(N, x_full_res.shape[2], x_full_res.shape[3], C).permute(0, 3, 1, 2)\n        # depth_emb = F.interpolate(depth_emb, size=(H, W), mode='bilinear', align_corners=False)\n        # depth_emb = depth_emb.permute(0, 2, 3, 1).reshape(N, H * W, C)\n        # depth_emb = self.dpe(depth_emb, (H, W))\n        # img_feat = x + self.img_conv(x)\n        # img_feat = img_feat.reshape(N, H * W, C)\n        # depth_feat = depth_feat.reshape(N, x_full_res.shape[2], x_full_res.shape[3], C).permute(0, 3, 1, 2)\n        # depth_feat = F.interpolate(depth_feat, size=(H, W), mode='bilinear', align_corners=False)\n        # depth_feat = depth_feat.permute(0, 2, 3, 1).reshape(N, H * W, C)\n        # feat = self.dtr(depth_feat, img_feat, depth_emb)\n        # feat = feat.permute(0, 2, 1).reshape(N, C, H, W)\n\n        return feat, depth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T14:46:29.136541Z","iopub.execute_input":"2025-08-19T14:46:29.137053Z","iopub.status.idle":"2025-08-19T14:46:29.151634Z","shell.execute_reply.started":"2025-08-19T14:46:29.137028Z","shell.execute_reply":"2025-08-19T14:46:29.150825Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"from visualDet3D.networks.heads.detection_3d_head import AnchorBasedDetection3DHead\nfrom visualDet3D.networks.heads.depth_losses import bin_depths, DepthFocalLoss\nfrom visualDet3D.networks.utils.registry import DETECTOR_DICT","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T14:46:29.152612Z","iopub.execute_input":"2025-08-19T14:46:29.153023Z","iopub.status.idle":"2025-08-19T14:46:29.171139Z","shell.execute_reply.started":"2025-08-19T14:46:29.152987Z","shell.execute_reply":"2025-08-19T14:46:29.170335Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"class MonoDTR(nn.Module):\n    def __init__(self, network_cfg):\n        super(MonoDTR, self).__init__()\n\n        self.obj_types = network_cfg.obj_types\n\n        self.build_head(network_cfg)\n\n        self.build_core(network_cfg)\n\n        self.network_cfg = network_cfg\n\n    def build_core(self, network_cfg):\n        self.mono_core = MonoDTRCore(network_cfg.mono_backbone)\n\n    def build_head(self, network_cfg):\n        self.bbox_head = AnchorBasedDetection3DHead(\n            **(network_cfg.head)\n        )\n        self.depth_loss = DepthFocalLoss(96)\n\n    def train_forward(self, left_images, annotations, P2, depth_gt=None):\n        \n        features, depth = self.mono_core(dict(image=left_images, P2=P2))\n        \n        depth_output   = depth\n        \n        features = features.contiguous()\n        P2 = P2.contiguous()\n        left_images = left_images.contiguous()\n        \n        try:\n            cls_preds, reg_preds = self.bbox_head(\n                dict(\n                    features=features,\n                    P2=P2,\n                    image=left_images\n                )\n            )\n        except RuntimeError as e:\n            print(f\"RuntimeError: {e}\")\n            raise\n            \n        anchors = self.bbox_head.get_anchor(left_images, P2)\n\n        cls_loss, reg_loss, loss_dict = self.bbox_head.loss(cls_preds, reg_preds, anchors, annotations, P2)\n        \n        depth_gt = bin_depths(depth_gt, mode = \"LID\", depth_min=1, depth_max=80, num_bins=96, target=True)\n\n        if reg_loss.mean() > 0 and not depth_gt is None and not depth_output is None:\n            \n            depth_gt = depth_gt.unsqueeze(1)\n            depth_loss = 1.0 * self.depth_loss(depth_output, depth_gt)\n            loss_dict['depth_loss'] = depth_loss\n            reg_loss += depth_loss\n\n            self.depth_output = depth_output.detach()\n        else:\n            loss_dict['depth_loss'] = torch.zeros_like(reg_loss)\n        return cls_loss, reg_loss, loss_dict\n\n    def test_forward(self, left_images, P2):\n        assert left_images.shape[0] == 1 # we recommmend image batch size = 1 for testing\n\n        features, _ = self.mono_core(dict(image=left_images, P2=P2))\n        \n        cls_preds, reg_preds = self.bbox_head(\n                dict(\n                    features=features,\n                    P2=P2,\n                    image=left_images\n                )\n            )\n\n        anchors = self.bbox_head.get_anchor(left_images, P2)\n\n        scores, bboxes, cls_indexes = self.bbox_head.get_bboxes(cls_preds, reg_preds, anchors, P2, left_images)\n        \n        return scores, bboxes, cls_indexes\n\n    def forward(self, inputs):\n\n        if isinstance(inputs, list) and len(inputs) >= 3:\n            return self.train_forward(*inputs)\n        else:\n            return self.test_forward(*inputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T14:46:29.173945Z","iopub.execute_input":"2025-08-19T14:46:29.174154Z","iopub.status.idle":"2025-08-19T14:46:29.185968Z","shell.execute_reply.started":"2025-08-19T14:46:29.174135Z","shell.execute_reply":"2025-08-19T14:46:29.185250Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"import os\nimport sys\nimport numpy as np\nfrom easydict import EasyDict\nfrom tqdm import tqdm\nfrom fire import Fire\nimport coloredlogs\nimport logging\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom _path_init import *\nfrom visualDet3D.networks.utils.registry import DETECTOR_DICT, DATASET_DICT, PIPELINE_DICT\nfrom visualDet3D.networks.utils.utils import BackProjection, BBox3dProjector, get_num_parameters\nfrom visualDet3D.evaluator.kitti.evaluate import evaluate\nimport visualDet3D.data.kitti.dataset\nfrom visualDet3D.utils.timer import Timer\nfrom visualDet3D.utils.utils import LossLogger, cfg_from_file\nfrom visualDet3D.networks.optimizers import optimizers, schedulers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T14:46:29.187095Z","iopub.execute_input":"2025-08-19T14:46:29.187383Z","iopub.status.idle":"2025-08-19T14:46:29.240634Z","shell.execute_reply.started":"2025-08-19T14:46:29.187353Z","shell.execute_reply":"2025-08-19T14:46:29.240056Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"from collections import OrderedDict  # Add this import\ndef main(config=\"/kaggle/input/monodtr-library-ver2/MonoDTR/config/config.py\", experiment_name=\"default\", world_size=1, local_rank=-1):\n    \"\"\"Main function for the training script.\n\n    KeywordArgs:\n        config (str): Path to config file.\n        experiment_name (str): Custom name for the experitment, only used in tensorboard.\n        world_size (int): Number of total subprocesses in distributed training. \n        local_rank: Rank of the process. Should not be manually assigned. 0-N for ranks in distributed training (only process 0 will print info and perform testing). -1 for single training. \n    \"\"\"\n\n    ## Get config\n    cfg = cfg_from_file(config)\n\n    ## Collect distributed(or not) information\n    cfg.dist = EasyDict()\n    cfg.dist.world_size = world_size\n    cfg.dist.local_rank = local_rank\n    is_distributed = local_rank >= 0 # local_rank < 0 -> single training\n    is_logging     = local_rank <= 0 # only log and test with main process\n    is_evaluating  = local_rank <= 0\n\n    ## Setup writer if local_rank > 0\n    recorder_dir = os.path.join(cfg.path.log_path, experiment_name + f\"config={config}\")\n    if is_logging: # writer exists only if not distributed and local rank is smaller\n        ## Clean up the dir if it exists before\n        if os.path.isdir(recorder_dir):\n            os.system(\"rm -r {}\".format(recorder_dir))\n            print(\"clean up the recorder directory of {}\".format(recorder_dir))\n        writer = SummaryWriter(recorder_dir)\n\n        ## Record config object using pprint\n        import pprint\n\n        formatted_cfg = pprint.pformat(cfg)\n        writer.add_text(\"config.py\", formatted_cfg.replace(' ', '&nbsp;').replace('\\n', '  \\n')) # add space for markdown style in tensorboard text\n    else:\n        writer = None\n\n    ## Set up GPU and distribution process\n    if is_distributed:\n        cfg.trainer.gpu = local_rank # local_rank will overwrite the GPU in configure file\n    gpu = min(cfg.trainer.gpu, torch.cuda.device_count() - 1)\n    torch.backends.cudnn.benchmark = getattr(cfg.trainer, 'cudnn', False)\n    torch.cuda.set_device(gpu)\n    if is_distributed:\n        torch.distributed.init_process_group(backend='nccl', init_method='env://')\n    print(local_rank)\n \n    ## define datasets and dataloader.\n    dataset_train = DATASET_DICT[cfg.data.train_dataset](cfg)\n    dataset_val = DATASET_DICT[cfg.data.val_dataset](cfg, \"validation\")\n\n    dataloader_train = DataLoader(dataset_train, num_workers=cfg.data.num_workers,\n                                  batch_size=cfg.data.batch_size, collate_fn=dataset_train.collate_fn, shuffle=local_rank<0, drop_last=True,\n                                  sampler=torch.utils.data.DistributedSampler(dataset_train, num_replicas=world_size, rank=local_rank, shuffle=True) if local_rank >= 0 else None)\n    dataloader_val = DataLoader(dataset_val, num_workers=cfg.data.num_workers,\n                                batch_size=cfg.data.batch_size, collate_fn=dataset_val.collate_fn, shuffle=False, drop_last=True)\n\n    ## Create the model\n    # detector = DETECTOR_DICT[cfg.detector.name](cfg.detector)\n    detector = MonoDTR(cfg.detector)\n\n    # # this is for load training checkpoint\n    state_dict_lasted = torch.load(\"/kaggle/input/checkpoint/MonoDTR_latest.pth\", map_location='cpu')\n    detector.load_state_dict(state_dict_lasted)\n    \n    # this is for load pretrain checkpoint\n    state_dict_pretrain = torch.load(\"/kaggle/input/pretrain-monodtr-base/MonoDTR.pth\", map_location='cpu')\n    detector.load_state_dict(state_dict_pretrain, strict=False)\n    \n    # Filter out incompatible keys\n    new_state_dict = OrderedDict()\n    loaded_keys = []  # Track which parameters are loaded\n    for k, v in state_dict_pretrain.items():\n        if k in detector.state_dict() and v.shape == detector.state_dict()[k].shape:\n            new_state_dict[k] = v\n            loaded_keys.append(k)  # Record loaded keys\n    \n    # detector.load_state_dict(new_state_dict, strict=False)\n    \n    # Freeze specific layers in the Swin-T backbone (first two stages)\n    for name, param in detector.named_parameters():\n        # print(name)\n        if name in loaded_keys:\n            param.requires_grad = False  # Freeze loaded weights\n        elif name.startswith('mono_core.backbone.layers_0'):\n        # elif name.startswith('mono_core.backbone.layers_0') or name.startswith('mono_core.backbone.layers_1') or name.startswith('mono_core.backbone.layers_2'):\n            param.requires_grad = True  # Freeze first two stages of Swin-T backbone\n        else:\n            param.requires_grad = False\n\n    for name, param in detector.named_parameters():\n        if param.requires_grad == True:\n            print(f\"Parameter {name} is trainable\")\n            \n            \n    ## Convert to cuda\n    if is_distributed:\n        detector = torch.nn.SyncBatchNorm.convert_sync_batchnorm(detector)\n        detector = torch.nn.parallel.DistributedDataParallel(detector.cuda(), device_ids=[gpu], output_device=gpu)\n    else:\n        detector = detector.cuda()\n    detector.train()\n\n    ## Record basic information of the model\n    if is_logging:\n        string1 = detector.__str__().replace(' ', '&nbsp;').replace('\\n', '  \\n')\n        writer.add_text(\"model structure\", string1) # add space for markdown style in tensorboard text\n        num_parameters = get_num_parameters(detector)\n        print(f'number of trained parameters of the model: {num_parameters}')\n    \n    ## define optimizer and weight decay\n    optimizer = optimizers.build_optimizer(cfg.optimizer, detector)\n\n    ## define scheduler\n    # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, cfg.trainer.max_epochs, cfg.optimizer.lr_target)\n    scheduler_config = getattr(cfg, 'scheduler', None)\n    scheduler = schedulers.build_scheduler(scheduler_config, optimizer)\n    is_iter_based = getattr(scheduler_config, \"is_iter_based\", False)\n\n    ## define loss logger\n    training_loss_logger =  LossLogger(writer, 'train') if is_logging else None\n\n    ## training pipeline\n    if 'training_func' in cfg.trainer:\n        training_dection = PIPELINE_DICT[cfg.trainer.training_func]\n    else:\n        raise KeyError\n\n    ## Get evaluation pipeline\n    if 'evaluate_func' in cfg.trainer:\n        evaluate_detection = PIPELINE_DICT[cfg.trainer.evaluate_func]\n        print(\"Found evaluate function {}\".format(cfg.trainer.evaluate_func))\n    else:\n        evaluate_detection = None\n        print(\"Evaluate function not found\")\n\n\n    ## timer is used to estimate eta\n    timer = Timer()\n\n    print('Num training images: {}'.format(len(dataset_train)))\n\n    global_step = 0\n    # for epoch_num in range(cfg.trainer.max_epochs):\n    for epoch_num in range(15):\n        ## Start training for one epoch\n        torch.cuda.empty_cache()\n        detector.train()\n        if training_loss_logger:\n            training_loss_logger.reset()\n        for iter_num, data in enumerate(dataloader_train):\n            training_dection(data, detector, optimizer, writer, training_loss_logger, global_step, epoch_num, cfg, iter_num)\n            # training_dection(data, detector, optimizer, writer, training_loss_logger, global_step, epoch_num, cfg)\n\n            global_step += 1\n            if (iter_num + 1) % cfg.trainer.accumulation_steps == 0:\n                if is_iter_based:\n                    scheduler.step()\n    \n                if is_logging and global_step % cfg.trainer.disp_iter == 0:\n                    ## Log loss, print out and write to tensorboard in main process\n                    if 'total_loss' not in training_loss_logger.loss_stats:\n                        print(f\"\\nIn epoch {epoch_num}, iteration:{iter_num}, global_step:{global_step}, total_loss not found in logger.\")\n                    else:\n                        log_str = 'Epoch: {} | Iteration: {}  | Running loss: {:1.5f} | eta:{}'.format(\n                            epoch_num, iter_num, training_loss_logger.loss_stats['total_loss'].avg,\n                            timer.compute_eta(global_step, len(dataloader_train) * cfg.trainer.max_epochs))\n                        print(log_str, end='\\r')\n                        writer.add_text(\"training_log/train\", log_str, global_step)\n                        training_loss_logger.log(global_step)\n\n        if not is_iter_based:\n            scheduler.step()\n        ## save model in main process if needed\n        if is_logging:\n            torch.save(detector.module.state_dict() if is_distributed else detector.state_dict(), os.path.join(\n                cfg.path.checkpoint_path, '{}_latest.pth'.format(\n                    cfg.detector.name)\n                )\n            )\n        if is_logging and (epoch_num + 1) % cfg.trainer.save_iter == 0:\n            torch.save(detector.module.state_dict() if is_distributed else detector.state_dict(), os.path.join(\n                cfg.path.checkpoint_path, '{}_{}.pth'.format(\n                    cfg.detector.name,epoch_num)\n                )\n            )\n\n        ## test model in main process if needed\n        # if is_evaluating and evaluate_detection is not None and cfg.trainer.test_iter > 0 and (epoch_num + 1) % cfg.trainer.test_iter == 0:\n        if is_evaluating and evaluate_detection is not None and cfg.trainer.test_iter > 0 and (epoch_num + 1) % 5 == 0:\n            print(\"\\n/**** start testing after training epoch {} ******/\".format(epoch_num))\n            evaluate_detection(cfg, detector.module if is_distributed else detector, dataset_val, writer, epoch_num)\n            print(\"/**** finish testing after training epoch {} ******/\".format(epoch_num))\n\n        if is_distributed:\n            torch.distributed.barrier() # wait untill all finish a epoch\n\n        if is_logging:\n            writer.flush()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T14:46:29.241363Z","iopub.execute_input":"2025-08-19T14:46:29.241674Z","iopub.status.idle":"2025-08-19T14:46:29.259214Z","shell.execute_reply.started":"2025-08-19T14:46:29.241644Z","shell.execute_reply":"2025-08-19T14:46:29.258277Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def run_training():\n    config_path = \"/kaggle/input/monodtr-library-ver2/MonoDTR/config/config.py\"  # Path to your config file\n    experiment_name = \"EXP_NAME\"  # Use the defined experiment name\n    world_size = 1  # For single GPU training\n    local_rank = 0  # Local rank set to 0 as per your command\n    %env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n    main(config=config_path, experiment_name=experiment_name, world_size=world_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T14:46:29.260004Z","iopub.execute_input":"2025-08-19T14:46:29.260277Z","iopub.status.idle":"2025-08-19T14:46:29.280162Z","shell.execute_reply.started":"2025-08-19T14:46:29.260255Z","shell.execute_reply":"2025-08-19T14:46:29.279311Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"run_training()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T14:46:29.280915Z","iopub.execute_input":"2025-08-19T14:46:29.281114Z","iopub.status.idle":"2025-08-19T14:46:35.135030Z","shell.execute_reply.started":"2025-08-19T14:46:29.281096Z","shell.execute_reply":"2025-08-19T14:46:35.133502Z"}},"outputs":[{"name":"stdout","text":"env: PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\nclean up the recorder directory of /kaggle/working/MonoDTR/log/EXP_NAMEconfig=/kaggle/input/monodtr-library-ver2/MonoDTR/config/config.py\n-1\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n<ipython-input-21-40052502e12b>:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict_lasted = torch.load(\"/kaggle/input/checkpoint/MonoDTR_latest.pth\", map_location='cpu')\n<ipython-input-21-40052502e12b>:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict_pretrain = torch.load(\"/kaggle/input/pretrain-monodtr-base/MonoDTR.pth\", map_location='cpu')\n","output_type":"stream"},{"name":"stdout","text":"Parameter mono_core.backbone.layers_0.blocks.0.norm1.weight is trainable\nParameter mono_core.backbone.layers_0.blocks.0.norm1.bias is trainable\nParameter mono_core.backbone.layers_0.blocks.0.attn.relative_position_bias_table is trainable\nParameter mono_core.backbone.layers_0.blocks.0.attn.qkv.weight is trainable\nParameter mono_core.backbone.layers_0.blocks.0.attn.qkv.bias is trainable\nParameter mono_core.backbone.layers_0.blocks.0.attn.proj.weight is trainable\nParameter mono_core.backbone.layers_0.blocks.0.attn.proj.bias is trainable\nParameter mono_core.backbone.layers_0.blocks.0.norm2.weight is trainable\nParameter mono_core.backbone.layers_0.blocks.0.norm2.bias is trainable\nParameter mono_core.backbone.layers_0.blocks.0.mlp.fc1.weight is trainable\nParameter mono_core.backbone.layers_0.blocks.0.mlp.fc1.bias is trainable\nParameter mono_core.backbone.layers_0.blocks.0.mlp.fc2.weight is trainable\nParameter mono_core.backbone.layers_0.blocks.0.mlp.fc2.bias is trainable\nParameter mono_core.backbone.layers_0.blocks.1.norm1.weight is trainable\nParameter mono_core.backbone.layers_0.blocks.1.norm1.bias is trainable\nParameter mono_core.backbone.layers_0.blocks.1.attn.relative_position_bias_table is trainable\nParameter mono_core.backbone.layers_0.blocks.1.attn.qkv.weight is trainable\nParameter mono_core.backbone.layers_0.blocks.1.attn.qkv.bias is trainable\nParameter mono_core.backbone.layers_0.blocks.1.attn.proj.weight is trainable\nParameter mono_core.backbone.layers_0.blocks.1.attn.proj.bias is trainable\nParameter mono_core.backbone.layers_0.blocks.1.norm2.weight is trainable\nParameter mono_core.backbone.layers_0.blocks.1.norm2.bias is trainable\nParameter mono_core.backbone.layers_0.blocks.1.mlp.fc1.weight is trainable\nParameter mono_core.backbone.layers_0.blocks.1.mlp.fc1.bias is trainable\nParameter mono_core.backbone.layers_0.blocks.1.mlp.fc2.weight is trainable\nParameter mono_core.backbone.layers_0.blocks.1.mlp.fc2.bias is trainable\nnumber of trained parameters of the model: 224694\nFound evaluate function evaluate_kitti_obj\nNum training images: 3712\n","output_type":"stream"},{"name":"stderr","text":"/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/data/kitti/dataset/mono_dataset.py:169: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  return torch.from_numpy(rgb_images).float(), torch.tensor(calib).float(), label, bbox2ds, bbox3ds, torch.tensor(depths).float()\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-2f18cc448813>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-22-b216eab3a1b0>\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlocal_rank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m  \u001b[0;31m# Local rank set to 0 as per your command\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'env'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexperiment_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworld_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworld_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-21-40052502e12b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(config, experiment_name, world_size, local_rank)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mtraining_loss_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0miter_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0mtraining_dection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_loss_logger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;31m# training_dection(data, detector, optimizer, writer, training_loss_logger, global_step, epoch_num, cfg)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/networks/pipelines/trainers.py\u001b[0m in \u001b[0;36mtrain_mono_detection\u001b[0;34m(data, module, optimizer, writer, loss_logger, global_step, epoch_num, cfg, iter_num)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# print(\"Calculate loss.backward()\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0miter_num\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulation_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# clip loss norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.96 GiB. GPU 0 has a total capacity of 15.89 GiB of which 581.12 MiB is free. Process 11803 has 15.32 GiB memory in use. Of the allocated memory 10.18 GiB is allocated by PyTorch, and 4.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 3.96 GiB. GPU 0 has a total capacity of 15.89 GiB of which 581.12 MiB is free. Process 11803 has 15.32 GiB memory in use. Of the allocated memory 10.18 GiB is allocated by PyTorch, and 4.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":23}]}