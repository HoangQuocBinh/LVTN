{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2255937,"sourceType":"datasetVersion","datasetId":1357458},{"sourceId":11368341,"sourceType":"datasetVersion","datasetId":7116330,"isSourceIdPinned":true},{"sourceId":11733251,"sourceType":"datasetVersion","datasetId":7365741},{"sourceId":12399365,"sourceType":"datasetVersion","datasetId":7381980},{"sourceId":12487699,"sourceType":"datasetVersion","datasetId":7110160}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!python --version","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:03.312421Z","iopub.execute_input":"2025-07-15T16:27:03.312702Z","iopub.status.idle":"2025-07-15T16:27:03.437416Z","shell.execute_reply.started":"2025-07-15T16:27:03.312678Z","shell.execute_reply":"2025-07-15T16:27:03.436572Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\n\nsys.path.append(\"/kaggle/input/monodtr-library-ver2/MonoDTR\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:03.438659Z","iopub.execute_input":"2025-07-15T16:27:03.438929Z","iopub.status.idle":"2025-07-15T16:27:03.442405Z","shell.execute_reply.started":"2025-07-15T16:27:03.438905Z","shell.execute_reply":"2025-07-15T16:27:03.441608Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -r /kaggle/input/monodtr-library-ver2/MonoDTR/requirement.txt\n!pip install coloredlogs\n!pip install ptflops","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:03.444048Z","iopub.execute_input":"2025-07-15T16:27:03.444271Z","iopub.status.idle":"2025-07-15T16:27:13.664990Z","shell.execute_reply.started":"2025-07-15T16:27:03.444245Z","shell.execute_reply":"2025-07-15T16:27:13.664142Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sys.path.append(\"/kaggle/input/monodtr-library-ver2/MonoDTR/scripts\")\nsys.path.append(\"/kaggle/input/pretrain-monodtr-base\")\nsys.path.append(\"/kaggle/input/checkpoint\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:13.666668Z","iopub.execute_input":"2025-07-15T16:27:13.666998Z","iopub.status.idle":"2025-07-15T16:27:13.670834Z","shell.execute_reply.started":"2025-07-15T16:27:13.666962Z","shell.execute_reply":"2025-07-15T16:27:13.669978Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport sys\nimport torch\n\n# Add the directory containing the .so file to the system path\nsys.path.append('/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/networks/lib/ops/dcn')\n\n# Check if the .so file exists\nprint(os.listdir('/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/networks/lib/ops/dcn'))\n\n# Print Python version\nprint(\"Python version:\", sys.version)\n\n# Set LD_LIBRARY_PATH\nos.environ['LD_LIBRARY_PATH'] = '/kaggle/input/monodtr-library-ver2/MonoDTR/visualDet3D/networks/lib/ops/dcn:' + os.environ.get('LD_LIBRARY_PATH', '')\n\n# Attempt to import the module\ntry:\n    import deform_conv_ext\n    print(\"Import successful!\")\nexcept ImportError as e:\n    print(\"Import failed:\", e)\n\n# If needed, run the setup or make script\n# !python setup.py build\n# !python setup.py install\n# or\n# !bash make.sh\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:13.671647Z","iopub.execute_input":"2025-07-15T16:27:13.671919Z","iopub.status.idle":"2025-07-15T16:27:15.291933Z","shell.execute_reply.started":"2025-07-15T16:27:13.671890Z","shell.execute_reply":"2025-07-15T16:27:15.290977Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\nif torch.cuda.is_available():\n    print(\"CUDA is available!\")\n    print(\"Number of GPUs available:\", torch.cuda.device_count())\n    print(\"Current GPU:\", torch.cuda.current_device())\n    print(\"GPU Name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\nelse:\n    print(\"CUDA is not available. Check your settings.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:15.292944Z","iopub.execute_input":"2025-07-15T16:27:15.293455Z","iopub.status.idle":"2025-07-15T16:27:15.343076Z","shell.execute_reply.started":"2025-07-15T16:27:15.293417Z","shell.execute_reply":"2025-07-15T16:27:15.342429Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a large tensor and move it to the GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nx = torch.randn(10000, 10000, device=device)  # This should allocate GPU memory\nprint(\"Tensor created on GPU.\")\nprint(\"Allocated GPU Memory:\", torch.cuda.memory_allocated(device))\nprint(\"Cached GPU Memory:\", torch.cuda.memory_reserved(device))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:15.343813Z","iopub.execute_input":"2025-07-15T16:27:15.344116Z","iopub.status.idle":"2025-07-15T16:27:15.457674Z","shell.execute_reply.started":"2025-07-15T16:27:15.344079Z","shell.execute_reply":"2025-07-15T16:27:15.456788Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn as nn\nimport torch\nimport torch\nfrom torch.nn import Module, Dropout\nfrom einops import rearrange\nimport torch.nn.functional as F\nfrom ptflops import get_model_complexity_info","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:15.460191Z","iopub.execute_input":"2025-07-15T16:27:15.460458Z","iopub.status.idle":"2025-07-15T16:27:17.578684Z","shell.execute_reply.started":"2025-07-15T16:27:15.460436Z","shell.execute_reply":"2025-07-15T16:27:17.577744Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------- RMSNorm --------------------------\nclass RMSNorm(nn.Module):\n    def __init__(self, dim, eps=1e-8):\n        super().__init__()\n        self.eps = eps\n        self.scale = nn.Parameter(torch.ones(dim))\n\n    def forward(self, x):\n        # x: (B, L, D)\n        norm_x = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n        return self.scale * norm_x\n\n# -------------------------- FLIP Module --------------------------\ndef flip(x, dim):\n    return torch.flip(x, dims=[dim])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:17.580045Z","iopub.execute_input":"2025-07-15T16:27:17.580334Z","iopub.status.idle":"2025-07-15T16:27:17.586792Z","shell.execute_reply.started":"2025-07-15T16:27:17.580311Z","shell.execute_reply":"2025-07-15T16:27:17.585731Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------- BiMamba2 Block (Conv Fusion) --------------------------\nclass BiMamba2(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.linear_x = nn.Linear(dim, dim)\n        self.linear_bf = nn.Linear(dim, dim)\n        self.linear_bb = nn.Linear(dim, dim)\n        self.linear_z = nn.Linear(dim, dim)\n\n        self.concat_proj_f = nn.Linear(2 * dim, dim)\n        self.concat_proj_b = nn.Linear(2 * dim, dim)\n\n        self.conv1d_f = nn.Conv1d(dim, dim, kernel_size=1)\n        self.conv1d_b = nn.Conv1d(dim, dim, kernel_size=1)\n        self.conv1d_zf = nn.Conv1d(dim, dim, kernel_size=1)\n        self.conv1d_ab_z = nn.Conv1d(dim, dim, kernel_size=1)\n\n        self.ssms_f = nn.Sequential(\n            nn.Conv1d(dim, dim, 1),\n            nn.GELU(),\n            nn.Conv1d(dim, dim, 1)\n        )\n        self.ssms_b = nn.Sequential(\n            nn.Conv1d(dim, dim, 1),\n            nn.GELU(),\n            nn.Conv1d(dim, dim, 1)\n        )\n\n        self.norm_f = RMSNorm(dim)\n        self.norm_b = RMSNorm(dim)\n        self.output_linear = nn.Linear(dim, dim)\n\n    def forward(self, u):\n        # u: (B, L, D)\n        x = self.linear_x(u)          # (B, L, D)\n        bf = self.linear_bf(u)        # (B, L, D)\n        bb = self.linear_bb(u)        # (B, L, D)\n        z = self.linear_z(u)          # (B, L, D)\n\n        bf_x = torch.cat([bf, x], dim=-1)         # (B, L, 2D)\n        bb_x = torch.cat([bb, x], dim=-1)         # (B, L, 2D)\n\n        bf_x = self.concat_proj_f(bf_x)           # (B, L, D)\n        bb_x = self.concat_proj_b(bb_x)           # (B, L, D)\n\n        af = self.conv1d_f(bf_x.transpose(1, 2))  # (B, D, L)\n        af = F.gelu(af)     \n        af = self.ssms_f(af).transpose(1, 2)      # (B, L, D)# (B, D, L)\n        # af = af + self.conv1d_zf(z.transpose(1, 2))  # af conv with z\n        af = af * z\n        af = self.norm_f(af)                      # (B, L, D)\n\n        bb_x_flip = flip(bb_x, dim=1)             # (B, L, D)\n        ab = self.conv1d_b(bb_x_flip.transpose(1, 2))  # (B, D, L)\n        ab = F.gelu(ab)                                # (B, D, L)\n\n        z_flip = flip(z, dim=1)                        # (B, L, D)\n        ab = self.ssms_b(ab).transpose(1, 2)           # (B, L, D)\n        # ab = ab + self.conv1d_ab_z(z_flip.transpose(1, 2))  # (B, D, L)\n        ab = ab * z_flip   # (B, D, L)\n        ab = self.norm_b(ab)                           # (B, L, D)\n        ab = flip(ab, dim=1)                           # (B, L, D)\n\n        out = self.output_linear(af + ab)         # (B, L, D)                 # (B, L, D)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:17.587648Z","iopub.execute_input":"2025-07-15T16:27:17.587956Z","iopub.status.idle":"2025-07-15T16:27:17.607255Z","shell.execute_reply.started":"2025-07-15T16:27:17.587933Z","shell.execute_reply":"2025-07-15T16:27:17.606312Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------- CrossMamba2 Module --------------------------\nclass CrossMamba2(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.linear_x = nn.Linear(dim, dim)\n        self.linear_z = nn.Linear(dim, dim)\n        self.linear_bf = nn.Linear(dim, dim)\n        self.linear_bb = nn.Linear(dim, dim)\n\n        self.concat_proj_f = nn.Linear(2 * dim, dim)\n        self.concat_proj_b = nn.Linear(2 * dim, dim)\n\n        self.conv1d_f = nn.Conv1d(dim, dim, kernel_size=1)\n        self.conv1d_b = nn.Conv1d(dim, dim, kernel_size=1)\n        self.conv1d_zf = nn.Conv1d(dim, dim, kernel_size=1)\n        self.conv1d_ab_z = nn.Conv1d(dim, dim, kernel_size=1)\n\n        self.ssms_f = nn.Sequential(\n            nn.Conv1d(dim, dim, 1),\n            nn.GELU(),\n            nn.Conv1d(dim, dim, 1)\n        )\n        self.ssms_b = nn.Sequential(\n            nn.Conv1d(dim, dim, 1),\n            nn.GELU(),\n            nn.Conv1d(dim, dim, 1)\n        )\n\n        self.norm_f = RMSNorm(dim)\n        self.norm_b = RMSNorm(dim)\n        self.output_linear = nn.Linear(dim, dim)\n\n    def forward(self, u1, u2):\n        # u1: context (B, L, D), u2: depth (B, L, D)\n        x = self.linear_x(u1)               # (B, L, D)\n        z = self.linear_z(u1)               # (B, L, D)\n        bf = self.linear_bf(u2)             # (B, L, D)\n        bb = self.linear_bb(u2)             # (B, L, D)\n\n        bf_x = torch.cat([bf, x], dim=-1)   # (B, L, 2D)\n        bb_x = torch.cat([bb, x], dim=-1)   # (B, L, 2D)\n\n        bf_x = self.concat_proj_f(bf_x)     # (B, L, D)\n        bb_x = self.concat_proj_b(bb_x)     # (B, L, D)\n\n        af = self.conv1d_f(bf_x.transpose(1, 2))  # (B, D, L)\n        af = F.gelu(af)\n        af = self.ssms_f(af).transpose(1, 2)      # (B, L, D)\n        # af = af + self.conv1d_zf(z.transpose(1, 2))  # af conv with z\n        af = af * z  # af conv with z\n        af = self.norm_f(af)\n\n        bb_x_flip = flip(bb_x, dim=1)             # (B, L, D)\n        ab = self.conv1d_b(bb_x_flip.transpose(1, 2))\n        ab = F.gelu(ab).transpose(1, 2)           # (B, L, D)\n\n        z_flip = flip(z, dim=1)\n        ab = self.ssms_b(ab.transpose(1, 2)).transpose(1, 2)\n        # ab = ab + self.conv1d_ab_z(z_flip.transpose(1, 2))  # (B, D, L)\n        ab = ab * z_flip  # (B, D, L)\n        ab = self.norm_b(ab)\n        ab = flip(ab, dim=1)\n\n        out = self.output_linear(af + ab)         # (B, L, D)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:17.608285Z","iopub.execute_input":"2025-07-15T16:27:17.608609Z","iopub.status.idle":"2025-07-15T16:27:17.628215Z","shell.execute_reply.started":"2025-07-15T16:27:17.608578Z","shell.execute_reply":"2025-07-15T16:27:17.627582Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------- FFN + Norm --------------------------\nclass FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),\n            nn.Linear(hidden_dim, dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:17.629065Z","iopub.execute_input":"2025-07-15T16:27:17.629336Z","iopub.status.idle":"2025-07-15T16:27:17.646972Z","shell.execute_reply.started":"2025-07-15T16:27:17.629312Z","shell.execute_reply":"2025-07-15T16:27:17.646171Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------- Depth Aware Mamba (DTR-Compatible Input) --------------------------\nclass DepthAwareMamba(nn.Module):\n    def __init__(self, output_channel_num):\n        super().__init__()\n        self.output_channel_num = output_channel_num\n\n        self.encoder_bimamba = BiMamba2(self.output_channel_num)\n        self.encoder_ffn = FeedForward(self.output_channel_num, self.output_channel_num * 2)\n        self.encoder_norm1 = nn.LayerNorm(self.output_channel_num)\n        self.encoder_norm2 = nn.LayerNorm(self.output_channel_num)\n\n        self.decoder_bimamba = BiMamba2(self.output_channel_num)\n        self.cross_bimamba = CrossMamba2(self.output_channel_num)\n        self.decoder_ffn = FeedForward(self.output_channel_num, self.output_channel_num * 2)\n        self.decoder_norm1 = nn.LayerNorm(self.output_channel_num)\n        self.decoder_norm2 = nn.LayerNorm(self.output_channel_num)\n        self.decoder_norm3 = nn.LayerNorm(self.output_channel_num)\n\n    def forward(self, depth_feat, context_feat, depth_pos=None):\n        depth_feat = depth_feat.contiguous()\n        context_feat = context_feat.contiguous()\n        if depth_pos is not None:\n            depth_pos = depth_pos.contiguous()\n            context_feat = context_feat + depth_pos\n    \n        # Encoder on context_feat\n        x = self.encoder_bimamba(context_feat.contiguous())\n        x = self.encoder_norm1((x + context_feat).contiguous())\n        x_ffn = self.encoder_ffn(x.contiguous())\n        x = self.encoder_norm2((x + x_ffn).contiguous())\n    \n        # Decoder on depth_feat and fused context\n        d = self.decoder_bimamba(depth_feat.contiguous())\n        d = self.decoder_norm1((d + depth_feat).contiguous())\n    \n        x = self.cross_bimamba(x.contiguous(), d.contiguous())\n        x = self.decoder_norm2(x.contiguous())\n        x_ffn = self.decoder_ffn(x.contiguous())\n        x = self.decoder_norm3((x + x_ffn).contiguous())\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:17.647817Z","iopub.execute_input":"2025-07-15T16:27:17.648089Z","iopub.status.idle":"2025-07-15T16:27:17.665759Z","shell.execute_reply.started":"2025-07-15T16:27:17.648071Z","shell.execute_reply":"2025-07-15T16:27:17.664933Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass CSA(nn.Module):\n    def __init__(self, in_channels):\n        super(CSA, self).__init__()\n        self.in_channels = in_channels\n\n    def forward(self, fine_feat, coarse_feat):\n        \"\"\"\n        Cross-Scale Attention (CSA) module.\n        \n        Inputs:\n            fine_feat: (N, C, H1, W1) - Finer resolution feature\n            coarse_feat: (N, C, H2, W2) - Coarser resolution feature (H2 < H1, W2 < W1)\n        \n        Output:\n            refined_feat: (N, C, H1, W1) - Refined feature at finer resolution\n        \"\"\"\n        N, C, H1, W1 = fine_feat.shape  # Finer resolution\n        _, _, H2, W2 = coarse_feat.shape  # Coarser resolution\n\n        # Upsample coarse features to match fine resolution\n        coarse_reshaped = F.interpolate(coarse_feat, size=(H1, W1), mode='bilinear', align_corners=False)\n        # coarse_reshaped: (N, C, H1, W1)\n\n        # Compute attention map\n        fine_flat = fine_feat.view(N, C, H1 * W1).permute(0, 2, 1)  # (N, H1*W1, C)\n        coarse_flat = coarse_reshaped.view(N, C, H1 * W1).permute(0, 2, 1)  # (N, H1*W1, C)\n        attention = torch.bmm(fine_flat, coarse_flat.transpose(1, 2))  # (N, H1*W1, H1*W1)\n        attention = F.softmax(attention, dim=-1)\n\n        # Apply attention to refine fine features\n        fine_attended = torch.bmm(attention, fine_flat).permute(0, 2, 1).view(N, C, H1, W1)\n        # fine_attended: (N, C, H1, W1)\n        refined_feat = fine_feat + fine_attended  # Residual connection\n        return refined_feat  # (N, C, H1, W1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:17.666469Z","iopub.execute_input":"2025-07-15T16:27:17.666739Z","iopub.status.idle":"2025-07-15T16:27:17.681847Z","shell.execute_reply.started":"2025-07-15T16:27:17.666708Z","shell.execute_reply":"2025-07-15T16:27:17.681088Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MSR(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(MSR, self).__init__()\n        self.mlp = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=1),\n            nn.ReLU(),\n            nn.Conv2d(out_channels, out_channels, kernel_size=1)\n        )\n\n    def forward(self, input_feat, target_size, combine_feat=None):\n        \"\"\"\n        Multi-Scale Refinement (MSR) module.\n        \n        Inputs:\n            input_feat: (N, C_in, H_in, W_in) - Feature to upsample and refine\n            target_size: tuple (H_target, W_target) - Target spatial dimensions\n            combine_feat: (N, C_out, H_target, W_target) or None - Feature to combine with\n        \n        Output:\n            refined_feat: (N, C_out, H_target, W_target)\n        \"\"\"\n        # Upsample input_feat to target_size\n        upsampled = F.interpolate(input_feat, size=target_size, mode='bilinear', align_corners=False)\n        # upsampled: (N, C_in, H_target, W_target)\n        \n        # Apply MLP to refine\n        refined = self.mlp(upsampled)\n        # refined: (N, C_out, H_target, W_target)\n        \n        if combine_feat is not None:\n            if combine_feat.shape[1] != refined.shape[1]:\n                raise ValueError(\"Channel mismatch for combination\")\n            refined = refined + combine_feat  # Additive combination\n            # refined: (N, C_out, H_target, W_target)\n        return refined","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:17.682559Z","iopub.execute_input":"2025-07-15T16:27:17.682745Z","iopub.status.idle":"2025-07-15T16:27:17.699859Z","shell.execute_reply.started":"2025-07-15T16:27:17.682729Z","shell.execute_reply":"2025-07-15T16:27:17.699134Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from visualDet3D.networks.backbones.dla import dla102\nfrom visualDet3D.networks.backbones.dlaup import DLAUp\nfrom visualDet3D.networks.detectors.dfe import DepthAwareFE\nfrom visualDet3D.networks.detectors.dpe import DepthAwarePosEnc\nfrom visualDet3D.networks.detectors.dtr import DepthAwareTransformer\nimport math\nimport time\nimport csv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:17.700485Z","iopub.execute_input":"2025-07-15T16:27:17.700704Z","iopub.status.idle":"2025-07-15T16:27:24.017025Z","shell.execute_reply.started":"2025-07-15T16:27:17.700686Z","shell.execute_reply":"2025-07-15T16:27:24.016368Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import timm\n\nclass MonoDTRCore(nn.Module):\n    def __init__(self, backbone_arguments=dict()):\n        super(MonoDTRCore, self).__init__()\n\n        # Swin-T backbone with features_only=True to ensure feature pyramid output\n        self.backbone = timm.create_model('swin_tiny_patch4_window7_224', pretrained=True,features_only=True, img_size=(288, 1280))\n        # self.backbone = timm.create_model('swin_tiny_patch4_window7_224', pretrained=True, features_only=True)\n        self.first_level = 1  # Kept for reference, but no longer used\n\n        # Feature pyramid channels from Swin-T\n        self.channels = [96, 192, 384, 768]\n\n        # Projection layers to adjust channels to 256 for CSA and MSR\n        self.proj_1_4 = nn.Conv2d(96, 256, kernel_size=1)\n        self.dropout_1_4 = nn.Dropout(0.3)\n        self.proj_1_8 = nn.Conv2d(192, 256, kernel_size=1)\n        self.dropout_1_8 = nn.Dropout(0.3)\n        self.proj_1_16 = nn.Conv2d(384, 256, kernel_size=1)\n        self.dropout_1_16 = nn.Dropout(0.3)\n        self.proj_1_32 = nn.Conv2d(768, 256, kernel_size=1)\n        self.dropout_1_32 = nn.Dropout(0.3)\n\n        # CSA modules for cross-scale attention\n        self.csa_1_16_to_1_32 = CSA(256)  # 1/16 with 1/32\n        self.csa_1_8_to_1_16 = CSA(256)   # 1/8 with 1/16\n        self.csa_1_4_to_1_8 = CSA(256)    # 1/4 with 1/8\n\n        # MSR modules for upsampling and refinement\n        self.msr_1_16_to_1_8 = MSR(256, 256)  # Upsample 1/16 to 1/8, combine with CSA_2 (1/8)\n        self.msr_1_8_to_1_4 = MSR(256, 256)   # Upsample 1/8 to 1/4, combine with CSA_3 (1/4)\n        self.msr_1_4_to_1_2 = MSR(256, 256)   # Upsample 1/4 to 1/2, no combine_feat (0 input)\n        self.msr_1_2_to_1_1 = MSR(256, 256)   # Upsample 1/2 to 1/1, no combine_feat (0 input)\n\n        # Downsample for transformer input to manage memory usage\n        self.downsample_for_transformer = nn.Conv2d(256, 256, kernel_size=3, stride=8, padding=1)\n        self.dropout_transformer = nn.Dropout(0.3)\n        # Downstream modules (assumed from original MonoDTRCore)\n        self.output_channel_num = 256\n        self.dpe = DepthAwarePosEnc(self.output_channel_num)\n        self.depth_embed = nn.Embedding(100, self.output_channel_num)\n        self.dtr = DepthAwareTransformer(self.output_channel_num)\n        self.dfe = DepthAwareFE(self.output_channel_num)\n        self.img_conv = nn.Conv2d(self.output_channel_num, self.output_channel_num, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of MonoDTRCore.\n        \n        Input:\n            x: dict with key 'image' containing (N, 3, H, W)\n        \n        Output:\n            feat: (N, C, H/8, W/8) - Processed feature map for compatibility with dtr\n            depth: (N, D, H, W) - Depth prediction at 1/1 resolution\n        \"\"\"\n        if 'image' not in x:\n            raise ValueError(\"Input dictionary must contain 'image' key\")\n        img = x['image']  # (N, 3, H, W)\n        assert img.shape[2] == 288 and img.shape[3] == 1280, f\"Expected shape [N, 3, 288, 1280], got {img.shape}\"\n\n        if img.dim() != 4:\n            raise ValueError(f\"Expected 4D input tensor (N, C, H, W), got shape {img.shape}\")\n        \n        if img.shape[1] != 3:\n            raise ValueError(\n                f\"Expected input image to have 3 channels (RGB), got {img.shape[1]} channels. \"\n                \"Please check your data pipeline to ensure the input is correctly formatted as (N, 3, H, W). \"\n                \"If the input has 7 channels (e.g., multi-spectral), preprocess it to 3 channels or modify the backbone.\"\n            )\n\n        # Extract features using standard forward method\n        features = self.backbone(img)  # [(N, H/4, W/4, 96), (N, H/8, W/8, 192), (N, H/16, W/16, 384), (N, H/32, W/32, 768)]\n        # print(f\"Feature shapes: {[f.shape for f in features]}\")  # Debug\n        # Permute features from (N, H, W, C) to (N, C, H, W)\n        features = [f.permute(0, 3, 1, 2) for f in features]  # [(N, 96, H/4, W/4), (N, 192, H/8, W/8), (N, 384, H/16, W/16), (N, 768, H/32, W/32)]\n\n        # Apply projection layers with dropout\n        proj_1_4 = self.dropout_1_4(self.proj_1_4(features[0]))   # (N, 256, 72, 320)\n        proj_1_8 = self.dropout_1_8(self.proj_1_8(features[1]))   # (N, 256, 36, 160)\n        proj_1_16 = self.dropout_1_16(self.proj_1_16(features[2])) # (N, 256, 18, 80)\n        proj_1_32 = self.dropout_1_32(self.proj_1_32(features[3])) # (N, 256, 9, 40)\n\n        # Refine features progressively with CSA and MSR\n        feat_1_16 = self.csa_1_16_to_1_32(proj_1_16, proj_1_32)  # (N, 256, H/16, W/16)\n        feat_1_8 = self.csa_1_8_to_1_16(proj_1_8, feat_1_16)     # (N, 256, H/8, W/8)\n        feat_1_4 = self.csa_1_4_to_1_8(proj_1_4, feat_1_8)       # (N, 256, H/4, W/4)\n        feat_1_8 = feat_1_8 + self.msr_1_16_to_1_8(feat_1_16, (proj_1_8.shape[2], proj_1_8.shape[3]), combine_feat=feat_1_8)  # (N, 256, H/8, W/8)\n        feat_1_4 = feat_1_4 + self.msr_1_8_to_1_4(feat_1_8, (proj_1_4.shape[2], proj_1_4.shape[3]), combine_feat=feat_1_4)      # (N, 256, H/4, W/4)\n        target_size_1_2 = (feat_1_4.shape[2] * 2, feat_1_4.shape[3] * 2)\n        feat_1_2 = self.msr_1_4_to_1_2(feat_1_4, target_size_1_2)  # (N, 256, H/2, W/2)\n        target_size_1_1 = (feat_1_2.shape[2] * 2, feat_1_2.shape[3] * 2)\n        feat_1_1 = self.msr_1_2_to_1_1(feat_1_2, target_size_1_1)  # (N, 256, H, W)\n\n        # Keep feat_1_1 for depth prediction at 1/1 resolution\n        x_full_res = feat_1_1  # (N, 256, H, W)\n\n        # Downsample for transformer to manage memory usage and match original resolution\n        x = self.downsample_for_transformer(x_full_res) # (N, 256, 36, 160)\n\n        # Proceed with downstream processing\n        N, C, H, W = x.shape  # H=H, W=W (full resolution)\n        depth, depth_guide, depth_feat = self.dfe(x) # depth: (N, D, 288, 1280), depth_guide: (N, num_classes, 288, 1280), depth_feat: (N, 256, 288, 1280)\n        depth_feat = depth_feat.permute(0, 2, 3, 1).view(N, H * W, C)\n        depth_guide = depth_guide.argmax(1)\n        depth_emb = self.depth_embed(depth_guide).view(N, H * W, C)\n        depth_emb = self.dpe(depth_emb, (H, W))\n        img_feat = x + self.img_conv(x)\n        img_feat = img_feat.permute(0, 2, 3, 1)\n        # print(f\"img_feat shape: {img_feat.shape}\")\n        img_feat = img_feat.view(N, H * W, C)\n        feat = self.dtr(depth_feat, img_feat, depth_emb)\n        feat = feat.permute(0, 2, 1).view(N, C, H, W)\n\n        # N, C, H, W = x.shape\n        # depth, depth_guide, depth_feat = self.dfe(x_full_res)\n        # depth_feat = depth_feat.permute(0, 2, 3, 1).reshape(N, (x_full_res.shape[2] * x_full_res.shape[3]), C)\n        # depth_guide = depth_guide.argmax(1)\n        # depth_emb = self.depth_embed(depth_guide).reshape(N, (x_full_res.shape[2] * x_full_res.shape[3]), C)\n        # depth_emb = depth_emb.reshape(N, x_full_res.shape[2], x_full_res.shape[3], C).permute(0, 3, 1, 2)\n        # depth_emb = F.interpolate(depth_emb, size=(H, W), mode='bilinear', align_corners=False)\n        # depth_emb = depth_emb.permute(0, 2, 3, 1).reshape(N, H * W, C)\n        # depth_emb = self.dpe(depth_emb, (H, W))\n        # img_feat = x + self.img_conv(x)\n        # img_feat = img_feat.reshape(N, H * W, C)\n        # depth_feat = depth_feat.reshape(N, x_full_res.shape[2], x_full_res.shape[3], C).permute(0, 3, 1, 2)\n        # depth_feat = F.interpolate(depth_feat, size=(H, W), mode='bilinear', align_corners=False)\n        # depth_feat = depth_feat.permute(0, 2, 3, 1).reshape(N, H * W, C)\n        # feat = self.dtr(depth_feat, img_feat, depth_emb)\n        # feat = feat.permute(0, 2, 1).reshape(N, C, H, W)\n\n        return feat, depth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:24.017809Z","iopub.execute_input":"2025-07-15T16:27:24.018361Z","iopub.status.idle":"2025-07-15T16:27:24.023352Z","shell.execute_reply.started":"2025-07-15T16:27:24.018324Z","shell.execute_reply":"2025-07-15T16:27:24.022460Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MonoDTRCore(nn.Module):\n    def __init__(self, backbone_arguments=dict()):\n        super(MonoDTRCore, self).__init__()\n        # self.backbone = dla102(pretrained=True, return_levels=True)\n        # channels = self.backbone.channels\n        # self.first_level = 3\n        \n        self.backbone = timm.create_model('swin_tiny_patch4_window7_224', pretrained=True,features_only=True, img_size=(288, 1280))\n        self.channels = [96, 192, 384, 768]\n        self.first_level = 1\n        # Freeze the backbone\n        # for param in self.backbone.parameters():  # Assuming 'backbone' is the attribute name\n        #     param.requires_grad = False\n\n        scales = [2**i for i in range(len(self.channels[self.first_level:]))]\n        self.neck = DLAUp(self.channels[self.first_level:], scales_list=scales)\n        # scales = [2**i for i in range(len(channels[self.first_level:]))]\n        # self.neck = DLAUp(channels[self.first_level:], scales_list=scales)\n        \n        # Projection layers to adjust channels to 256 for CSA and MSR\n        self.proj_1_8 = nn.Conv2d(192, 256, kernel_size=1)\n        \n        self.output_channel_num = 256\n                \n        self.dpe = DepthAwarePosEnc(self.output_channel_num)\n        for param in self.dpe.parameters():  # Assuming 'backbone' is the attribute name\n            param.requires_grad = False\n            \n        self.depth_embed = nn.Embedding(100, self.output_channel_num)\n        # self.dtr = DepthAwareMamba(self.output_channel_num)\n        self.dtr = DepthAwareTransformer(self.output_channel_num)\n        for param in self.dtr.parameters():  # Assuming 'backbone' is the attribute name\n            param.requires_grad = False\n            \n        self.dfe = DepthAwareFE(self.output_channel_num)\n        for param in self.dfe.parameters():  # Assuming 'backbone' is the attribute name\n            param.requires_grad = False\n            \n        self.img_conv = nn.Conv2d(self.output_channel_num, self.output_channel_num, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        x = self.backbone(x['image'])\n        # with torch.cuda.device(0):  # chỉ cần dùng một GPU để tính FLOPs\n        #     macs, params = get_model_complexity_info(\n        #         self.backbone, (3, 512, 512),\n        #         as_strings=True,\n        #         print_per_layer_stat=False,\n        #         verbose=False)\n        # print(f\"FLOPs: {macs}\")\n        x = [x_index.permute(0, 3, 1, 2) for x_index in x]\n        x = self.neck(x[self.first_level:])\n        x = self.proj_1_8(x)\n        # x = self.backbone_neck(x['image'])\n        \n        N, C, H, W = x.shape\n        depth, depth_guide, depth_feat = self.dfe(x)\n        \n        depth_feat = depth_feat.permute(0, 2, 3, 1).view(N, H*W, C)\n        \n        depth_guide = depth_guide.argmax(1)\n        depth_emb = self.depth_embed(depth_guide).view(N, H*W, C)\n        depth_emb = self.dpe(depth_emb, (H,W))\n        \n        img_feat = x + self.img_conv(x)\n        img_feat = img_feat.reshape(N, H * W, C)\n        # img_feat = img_feat.view(N, H*W, C)\n        feat = self.dtr(depth_feat, img_feat, depth_emb)\n        feat = feat.permute(0, 2, 1).view(N,C,H,W)\n        return feat, depth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:24.024408Z","iopub.execute_input":"2025-07-15T16:27:24.024696Z","iopub.status.idle":"2025-07-15T16:27:24.049044Z","shell.execute_reply.started":"2025-07-15T16:27:24.024667Z","shell.execute_reply":"2025-07-15T16:27:24.048396Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from visualDet3D.networks.heads.detection_3d_head import AnchorBasedDetection3DHead\nfrom visualDet3D.networks.heads.depth_losses import bin_depths, DepthFocalLoss\nfrom visualDet3D.networks.utils.registry import DETECTOR_DICT","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:24.049758Z","iopub.execute_input":"2025-07-15T16:27:24.049951Z","iopub.status.idle":"2025-07-15T16:27:24.067563Z","shell.execute_reply.started":"2025-07-15T16:27:24.049934Z","shell.execute_reply":"2025-07-15T16:27:24.066908Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MonoDTR(nn.Module):\n    def __init__(self, network_cfg):\n        super(MonoDTR, self).__init__()\n\n        self.obj_types = network_cfg.obj_types\n\n        self.build_head(network_cfg)\n\n        self.build_core(network_cfg)\n\n        self.network_cfg = network_cfg\n\n    def build_core(self, network_cfg):\n        self.mono_core = MonoDTRCore(network_cfg.mono_backbone)\n\n    def build_head(self, network_cfg):\n        self.bbox_head = AnchorBasedDetection3DHead(\n            **(network_cfg.head)\n        )\n        self.depth_loss = DepthFocalLoss(96)\n\n    def train_forward(self, left_images, annotations, P2, depth_gt=None):\n        \n        features, depth = self.mono_core(dict(image=left_images, P2=P2))\n        \n        depth_output   = depth\n        \n        features = features.contiguous()\n        P2 = P2.contiguous()\n        left_images = left_images.contiguous()\n        \n        try:\n            cls_preds, reg_preds = self.bbox_head(\n                dict(\n                    features=features,\n                    P2=P2,\n                    image=left_images\n                )\n            )\n        except RuntimeError as e:\n            print(f\"RuntimeError: {e}\")\n            raise\n            \n        anchors = self.bbox_head.get_anchor(left_images, P2)\n\n        cls_loss, reg_loss, loss_dict = self.bbox_head.loss(cls_preds, reg_preds, anchors, annotations, P2)\n        \n        depth_gt = bin_depths(depth_gt, mode = \"LID\", depth_min=1, depth_max=80, num_bins=96, target=True)\n\n        if reg_loss.mean() > 0 and not depth_gt is None and not depth_output is None:\n            \n            depth_gt = depth_gt.unsqueeze(1)\n            depth_loss = 1.0 * self.depth_loss(depth_output, depth_gt)\n            loss_dict['depth_loss'] = depth_loss\n            reg_loss += depth_loss\n\n            self.depth_output = depth_output.detach()\n        else:\n            loss_dict['depth_loss'] = torch.zeros_like(reg_loss)\n        return cls_loss, reg_loss, loss_dict\n\n    def test_forward(self, left_images, P2):\n        assert left_images.shape[0] == 1 # we recommmend image batch size = 1 for testing\n\n        features, _ = self.mono_core(dict(image=left_images, P2=P2))\n        \n        cls_preds, reg_preds = self.bbox_head(\n                dict(\n                    features=features,\n                    P2=P2,\n                    image=left_images\n                )\n            )\n\n        anchors = self.bbox_head.get_anchor(left_images, P2)\n\n        scores, bboxes, cls_indexes = self.bbox_head.get_bboxes(cls_preds, reg_preds, anchors, P2, left_images)\n        \n        return scores, bboxes, cls_indexes\n\n    def forward(self, inputs):\n\n        if isinstance(inputs, list) and len(inputs) >= 3:\n            return self.train_forward(*inputs)\n        else:\n            return self.test_forward(*inputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:24.068294Z","iopub.execute_input":"2025-07-15T16:27:24.068582Z","iopub.status.idle":"2025-07-15T16:27:24.087014Z","shell.execute_reply.started":"2025-07-15T16:27:24.068549Z","shell.execute_reply":"2025-07-15T16:27:24.086177Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport sys\nimport numpy as np\nfrom easydict import EasyDict\nfrom tqdm import tqdm\nfrom fire import Fire\nimport coloredlogs\nimport logging\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom _path_init import *\nfrom visualDet3D.networks.utils.registry import DETECTOR_DICT, DATASET_DICT, PIPELINE_DICT\nfrom visualDet3D.networks.utils.utils import BackProjection, BBox3dProjector, get_num_parameters\nfrom visualDet3D.evaluator.kitti.evaluate import evaluate\nimport visualDet3D.data.kitti.dataset\nfrom visualDet3D.utils.timer import Timer\nfrom visualDet3D.utils.utils import LossLogger, cfg_from_file\nfrom visualDet3D.networks.optimizers import optimizers, schedulers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:24.087808Z","iopub.execute_input":"2025-07-15T16:27:24.088075Z","iopub.status.idle":"2025-07-15T16:27:24.131958Z","shell.execute_reply.started":"2025-07-15T16:27:24.088045Z","shell.execute_reply":"2025-07-15T16:27:24.131399Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import OrderedDict  # Add this import\ndef main(config=\"/kaggle/input/monodtr-library-ver2/MonoDTR/config/config.py\", experiment_name=\"default\", world_size=1, local_rank=-1):\n    \"\"\"Main function for the training script.\n\n    KeywordArgs:\n        config (str): Path to config file.\n        experiment_name (str): Custom name for the experitment, only used in tensorboard.\n        world_size (int): Number of total subprocesses in distributed training. \n        local_rank: Rank of the process. Should not be manually assigned. 0-N for ranks in distributed training (only process 0 will print info and perform testing). -1 for single training. \n    \"\"\"\n\n    ## Get config\n    cfg = cfg_from_file(config)\n\n    ## Collect distributed(or not) information\n    cfg.dist = EasyDict()\n    cfg.dist.world_size = world_size\n    cfg.dist.local_rank = local_rank\n    is_distributed = local_rank >= 0 # local_rank < 0 -> single training\n    is_logging     = local_rank <= 0 # only log and test with main process\n    is_evaluating  = local_rank <= 0\n\n    ## Setup writer if local_rank > 0\n    recorder_dir = os.path.join(cfg.path.log_path, experiment_name + f\"config={config}\")\n    if is_logging: # writer exists only if not distributed and local rank is smaller\n        ## Clean up the dir if it exists before\n        if os.path.isdir(recorder_dir):\n            os.system(\"rm -r {}\".format(recorder_dir))\n            print(\"clean up the recorder directory of {}\".format(recorder_dir))\n        writer = SummaryWriter(recorder_dir)\n\n        ## Record config object using pprint\n        import pprint\n\n        formatted_cfg = pprint.pformat(cfg)\n        writer.add_text(\"config.py\", formatted_cfg.replace(' ', '&nbsp;').replace('\\n', '  \\n')) # add space for markdown style in tensorboard text\n    else:\n        writer = None\n\n    ## Set up GPU and distribution process\n    if is_distributed:\n        cfg.trainer.gpu = local_rank # local_rank will overwrite the GPU in configure file\n    gpu = min(cfg.trainer.gpu, torch.cuda.device_count() - 1)\n    torch.backends.cudnn.benchmark = getattr(cfg.trainer, 'cudnn', False)\n    torch.cuda.set_device(gpu)\n    if is_distributed:\n        torch.distributed.init_process_group(backend='nccl', init_method='env://')\n    print(local_rank)\n \n    ## define datasets and dataloader.\n    dataset_train = DATASET_DICT[cfg.data.train_dataset](cfg)\n    dataset_val = DATASET_DICT[cfg.data.val_dataset](cfg, \"validation\")\n\n    dataloader_train = DataLoader(dataset_train, num_workers=cfg.data.num_workers,\n                                  batch_size=cfg.data.batch_size, collate_fn=dataset_train.collate_fn, shuffle=local_rank<0, drop_last=True,\n                                  sampler=torch.utils.data.DistributedSampler(dataset_train, num_replicas=world_size, rank=local_rank, shuffle=True) if local_rank >= 0 else None)\n    dataloader_val = DataLoader(dataset_val, num_workers=cfg.data.num_workers,\n                                batch_size=cfg.data.batch_size, collate_fn=dataset_val.collate_fn, shuffle=False, drop_last=True)\n\n    ## Create the model\n    # detector = DETECTOR_DICT[cfg.detector.name](cfg.detector)\n    detector = MonoDTR(cfg.detector)\n    # print(detector.state_dict())\n    ## Load old model if needed\n    # old_checkpoint = getattr(cfg.path, 'pretrained_checkpoint', None)\n    # if old_checkpoint is not None:\n\n    # this is for load pretrain checkpoint\n    state_dict = torch.load(\"/kaggle/input/pretrain-monodtr-base/MonoDTR.pth\", map_location='cpu')\n    \n    # # this is for load training checkpoint\n    # state_dict = torch.load(\"/kaggle/input/checkpoint/MonoDTR_latest.pth\", map_location='cpu')\n    # detector.load_state_dict(state_dict)\n    \n    # Filter out incompatible keys\n    new_state_dict = OrderedDict()\n    loaded_keys = []  # Track which parameters are loaded\n    for k, v in state_dict.items():\n        if k.startswith('mono_core.backbone.'):\n    #         # print(f\"Skipping backbone parameter: {k}, checkpoint shape: {v.shape}\")\n            continue  # Skip all backbone parameters\n        # if k.startswith('mono_core.neck.'):\n            # print(f\"Skipping neck parameter: {k}, checkpoint shape: {v.shape}\")\n            # continue  # Skip all neck parameters\n        if k in detector.state_dict() and v.shape == detector.state_dict()[k].shape:\n            new_state_dict[k] = v\n            loaded_keys.append(k)  # Record loaded keys\n        # else:\n        #     print(f\"Skipping incompatible parameter: {k}, checkpoint shape: {v.shape}, model shape: {detector.state_dict()[k].shape}\")\n    \n    # detector.load_state_dict(new_state_dict, strict=False)\n    \n    # Freeze specific layers in the Swin-T backbone (first two stages)\n    for name, param in detector.named_parameters():\n        # print(name)\n        if name in loaded_keys:\n            param.requires_grad = False  # Freeze loaded weights\n            # print(f\"Freezing parameter: {name}\")\n        elif name.startswith('mono_core.backbone.layers_0') or name.startswith('mono_core.backbone.layers_1') or name.startswith('mono_core.backbone.layers_2'):\n            param.requires_grad = False  # Freeze first two stages of Swin-T backbone\n            # print(f\"Freezing backbone parameter (early stage): {name}\")\n    #     # else:\n    #         # print(f\"Parameter {name} is trainable (e.g., later backbone stages or neck)\")\n\n    \n    # # detector.load_state_dict(state_dict, strict=False)\n\n\n    ## Convert to cuda\n    if is_distributed:\n        detector = torch.nn.SyncBatchNorm.convert_sync_batchnorm(detector)\n        detector = torch.nn.parallel.DistributedDataParallel(detector.cuda(), device_ids=[gpu], output_device=gpu)\n    else:\n        detector = detector.cuda()\n    detector.train()\n\n    ## Record basic information of the model\n    if is_logging:\n        string1 = detector.__str__().replace(' ', '&nbsp;').replace('\\n', '  \\n')\n        writer.add_text(\"model structure\", string1) # add space for markdown style in tensorboard text\n        num_parameters = get_num_parameters(detector)\n        print(f'number of trained parameters of the model: {num_parameters}')\n    \n    ## define optimizer and weight decay\n    optimizer = optimizers.build_optimizer(cfg.optimizer, detector)\n\n    ## define scheduler\n    # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, cfg.trainer.max_epochs, cfg.optimizer.lr_target)\n    scheduler_config = getattr(cfg, 'scheduler', None)\n    scheduler = schedulers.build_scheduler(scheduler_config, optimizer)\n    is_iter_based = getattr(scheduler_config, \"is_iter_based\", False)\n\n    ## define loss logger\n    training_loss_logger =  LossLogger(writer, 'train') if is_logging else None\n\n    ## training pipeline\n    if 'training_func' in cfg.trainer:\n        training_dection = PIPELINE_DICT[cfg.trainer.training_func]\n    else:\n        raise KeyError\n\n    ## Get evaluation pipeline\n    if 'evaluate_func' in cfg.trainer:\n        evaluate_detection = PIPELINE_DICT[cfg.trainer.evaluate_func]\n        print(\"Found evaluate function {}\".format(cfg.trainer.evaluate_func))\n    else:\n        evaluate_detection = None\n        print(\"Evaluate function not found\")\n\n\n    ## timer is used to estimate eta\n    timer = Timer()\n\n    print('Num training images: {}'.format(len(dataset_train)))\n\n    global_step = 0\n    running_loss = 0.0\n    for epoch_num in range(cfg.trainer.max_epochs):\n    # for epoch_num in range(20):\n        ## Start training for one epoch\n        detector.train()\n        if training_loss_logger:\n            training_loss_logger.reset()\n        for iter_num, data in enumerate(dataloader_train):\n            training_dection(data, detector, optimizer, writer, training_loss_logger, global_step, epoch_num, cfg)\n\n            global_step += 1\n\n            if is_iter_based:\n                scheduler.step()\n\n            if is_logging and global_step % cfg.trainer.disp_iter == 0:\n                ## Log loss, print out and write to tensorboard in main process\n                if 'total_loss' not in training_loss_logger.loss_stats:\n                    print(f\"\\nIn epoch {epoch_num}, iteration:{iter_num}, global_step:{global_step}, total_loss not found in logger.\")\n                else:\n                    log_str = 'Epoch: {} | Iteration: {}  | Running loss: {:1.5f} | eta:{}'.format(\n                        epoch_num, iter_num, training_loss_logger.loss_stats['total_loss'].avg,\n                        timer.compute_eta(global_step, len(dataloader_train) * cfg.trainer.max_epochs))\n                    print(log_str, end='\\r')\n                    writer.add_text(\"training_log/train\", log_str, global_step)\n                    training_loss_logger.log(global_step)\n\n        if not is_iter_based:\n            scheduler.step()\n        ## save model in main process if needed\n        if is_logging:\n            torch.save(detector.module.state_dict() if is_distributed else detector.state_dict(), os.path.join(\n                cfg.path.checkpoint_path, '{}_latest.pth'.format(\n                    cfg.detector.name)\n                )\n            )\n        if is_logging and (epoch_num + 1) % cfg.trainer.save_iter == 0:\n            torch.save(detector.module.state_dict() if is_distributed else detector.state_dict(), os.path.join(\n                cfg.path.checkpoint_path, '{}_{}.pth'.format(\n                    cfg.detector.name,epoch_num)\n                )\n            )\n\n        ## test model in main process if needed\n        if is_evaluating and evaluate_detection is not None and cfg.trainer.test_iter > 0 and (epoch_num + 1) % cfg.trainer.test_iter == 0:\n            print(\"\\n/**** start testing after training epoch {} ******/\".format(epoch_num))\n            evaluate_detection(cfg, detector.module if is_distributed else detector, dataset_val, writer, epoch_num)\n            print(\"/**** finish testing after training epoch {} ******/\".format(epoch_num))\n\n        if is_distributed:\n            torch.distributed.barrier() # wait untill all finish a epoch\n\n        if is_logging:\n            writer.flush()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:24.132705Z","iopub.execute_input":"2025-07-15T16:27:24.132895Z","iopub.status.idle":"2025-07-15T16:27:24.149857Z","shell.execute_reply.started":"2025-07-15T16:27:24.132878Z","shell.execute_reply":"2025-07-15T16:27:24.148959Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def run_training():\n    config_path = \"/kaggle/input/monodtr-library-ver2/MonoDTR/config/config.py\"  # Path to your config file\n    experiment_name = \"EXP_NAME\"  # Use the defined experiment name\n    world_size = 1  # For single GPU training\n    local_rank = 0  # Local rank set to 0 as per your command\n\n    main(config=config_path, experiment_name=experiment_name, world_size=world_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:24.152547Z","iopub.execute_input":"2025-07-15T16:27:24.152758Z","iopub.status.idle":"2025-07-15T16:27:24.167471Z","shell.execute_reply.started":"2025-07-15T16:27:24.152739Z","shell.execute_reply":"2025-07-15T16:27:24.166738Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"run_training()/kaggle/input/monodtr-dataset/MonoDTR/output/validation/imdb.pkl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T16:27:24.168200Z","iopub.execute_input":"2025-07-15T16:27:24.168485Z","iopub.status.idle":"2025-07-15T16:28:43.301886Z","shell.execute_reply.started":"2025-07-15T16:27:24.168453Z","shell.execute_reply":"2025-07-15T16:28:43.300349Z"}},"outputs":[],"execution_count":null}]}